{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PySpark (SDDM, 2018-2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of PySpark\n",
    "\n",
    "- Apache Spark is written in Scala\n",
    "\n",
    "- To support Python with Spark, Apache Spark Community released PySpark\n",
    "\n",
    "- Similar computation speed and power as Scala\n",
    "\n",
    "- PySpark APIs are similar to Pandas and Scikit-learn\n",
    "\n",
    "- The high level components of a Spark application include the Spark driver, the Spark executors and the Cluster Manager.\n",
    "\n",
    "- Spark supports three cluster managers:\n",
    "\n",
    "    - Built-in standalone cluster managers\n",
    "    - Apache Mesos\n",
    "    - Hadoop YARN\n",
    "\n",
    "- Execution modes:\n",
    "\n",
    "    - Cluster Mode, \n",
    "    - Client Mode(default),\n",
    "    - Local Mode.\n",
    "\n",
    "### What is Spark shell?\n",
    "\n",
    "- Interactive environment for running Spark jobs\n",
    "\n",
    "- Helpful for fast interactive prototyping\n",
    "\n",
    "- Spark’s shells allow interacting with data on disk or in memory\n",
    "\n",
    "- Three different Spark shells:\n",
    "\n",
    "    Spark-shell for Scala\n",
    "\n",
    "    PySpark-shell for Python\n",
    "\n",
    "    SparkR for R\n",
    "\n",
    "\n",
    "### PySpark shell\n",
    "\n",
    "- PySpark shell is the Python-based command line tool\n",
    "\n",
    "- PySpark shell data scientists interfere with Spark data structures\n",
    "\n",
    "- PySpark shell support connecting to cluster\n",
    "\n",
    "\n",
    "### Understanding SparkContext\n",
    "\n",
    "- SparkContext is an entry point into the world of Spark\n",
    "\n",
    "- An entry point is a way of connecting to Spark cluster\n",
    "\n",
    "- An entry point is like a key to the house\n",
    "\n",
    "- PySpark has a default SparkContext called sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO SETUP DAS3 FOR SPARK\n",
    "\n",
    "(steps are similar in DSLab Machines but there is no clustermode over there.. master = local[*])\n",
    "\n",
    "this instruction will be for standalone mode\n",
    "\n",
    "other modes : yarn, mesos etc.\n",
    "\n",
    "ssh kocamanv@gold.liacs.nl\n",
    "\n",
    "ssh kocamanv@fs.das3.liacs.nl\n",
    "\n",
    "download Spark (only on master node.. as your local will be same in each node)\n",
    "\n",
    "wget http://apache.mirror.h1.nl/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "\n",
    "tar -xzf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "\n",
    "cd spark-2.4.0-bin-hadoop2.7\n",
    "\n",
    "check /etc/hosts to get nodenames per ip\n",
    "\n",
    "add the workers to slaves list via \"nano /conf/slaves\" or \"sinfo\"\n",
    "\n",
    "node05.das3.liacs.nl\n",
    "node07.das3.liacs.nl\n",
    "node08.das3.liacs.nl\n",
    "\n",
    "add the following line to spark-env.sh via \"nano /conf/spark-env.sh\"\n",
    "\n",
    "get the ip address \n",
    "\n",
    "ip  -f inet a show eth0| grep inet| awk '{ print $2}' | cut -d/ -f1\n",
    "\n",
    "\n",
    "SPARK_MASTER_HOST=\"132.229.135.12\"\n",
    "\n",
    "\n",
    "passwordless-ssh to nodes\n",
    "\n",
    "ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub kocamanv@node05.das3.liacs.nl\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub kocamanv@node07.das3.liacs.nl\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub kocamanv@node08.das3.liacs.nl\n",
    "\n",
    "\n",
    "start master node\n",
    "\n",
    "./sbin/start-master.sh\n",
    "\n",
    "start slaves\n",
    "\n",
    "./sbin/start-slaves.sh\n",
    "\n",
    "stop all (run on master)\n",
    "\n",
    "./sbin/stop-all.sh\n",
    "\n",
    "\n",
    "Spark web UI\n",
    "\n",
    "ssh -L 7891:localhost:2347 kocamanv@gold.liacs.nl\n",
    "ssh -L 2347:localhost:8081 kocamanv@fs.das3.liacs.nl\n",
    "        \n",
    "\n",
    "8080 of das3 is reserved for Ambari. So we use 8081\n",
    "\n",
    "go to localhost:7891 to see Spark Web UI\n",
    "\n",
    "        \n",
    "Spark jobs UI\n",
    "\n",
    "ssh -L 7893:localhost:2349 kocamanv@gold.liacs.nl\n",
    "ssh -L 2349:localhost:4040 kocamanv@fs.das3.liacs.nl\n",
    "        \n",
    "\n",
    "jupyter notebook --no-browser --port=9995\n",
    "        \n",
    "when you run any job, go to localhost:7893 to see Spark Jobs UI\n",
    "\n",
    "get the number of executers and cores\n",
    "\n",
    "print (sc._jsc.sc().getExecutorMemoryStatus())\n",
    "\n",
    "submit Sparkjobs\n",
    "\n",
    "./bin/spark-submit --driver-memory 4g --executor-memory 2G --master spark://fs.das3.liacs.nl:7077 sample_pyspark.py\n",
    "\n",
    "\n",
    "### how to install pyspark on jupyter on local\n",
    "\n",
    "https://medium.com/spark-nlp/introduction-to-spark-nlp-installation-and-getting-started-part-ii-d009f7a177f3\n",
    "\n",
    "download pyspark from http://spark.apache.org/downloads.html\n",
    "(no need for pip install pyspark.. it's already bundled with spark)\n",
    "pip install py4j\n",
    "\n",
    "https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f\n",
    "\n",
    "(remove newer versions of java.. keep jdk 8.x)\n",
    "\n",
    "no need for findspark\n",
    "\n",
    "for windows:\n",
    "\n",
    "https://medium.com/@ashish1512/how-to-setup-apache-spark-pyspark-on-jupyter-ipython-notebook-3330543ab307\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A functional approach to programming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want to square each term in my_list.\n",
    "\n",
    "squared_list = map(lambda x:x**2,my_list)\n",
    "\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, you could think of map as a function which takes two arguments — A function and a list.\n",
    "\n",
    "It then applies the function to every element of the list.\n",
    "\n",
    "What lambda allows you to do is write an inline function. In here the part lambda x:x**2 defines a function that takes x as input and returns x².\n",
    "\n",
    "You could have also provided a proper function in place of lambda. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want to square each term in my_list.\n",
    "\n",
    "squared_list = map(squared,my_list)\n",
    "\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result, but the lambda expressions make the code compact and a lot more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# This function takes two arguments — A condition and the list to filter.\n",
    "# If you want to filter your list using some condition you use filter.\n",
    "\n",
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want only the even numbers in my list.\n",
    "\n",
    "filtered_list = filter(lambda x:x%2==0,my_list)\n",
    "\n",
    "print(list(filtered_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#This function will be the workhorse in Spark.\n",
    "#This function takes two arguments — a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.\n",
    "\n",
    "import functools\n",
    "\n",
    "my_list = [1,2,3,4,5]\n",
    "\n",
    "# Lets say I want to sum all elements in my list.\n",
    "\n",
    "sum_list = functools.reduce(lambda x,y:x+y, my_list)\n",
    "\n",
    "print(sum_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:\n",
    "\n",
    "Reduce function first sends 1,2    ; the lambda function returns 3\n",
    "\n",
    "Reduce function then sends 3,3     ; the lambda function returns 6\n",
    "\n",
    "Reduce function then sends 6,4     ; the lambda function returns 10\n",
    "\n",
    "Reduce function finally sends 10,5 ; the lambda function returns 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS NOTEBOOK IS DESIGNED FOR RUNNING PYSPARK ON LOCAL MODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import warnings\n",
    "\n",
    "sc = SparkContext(appName=\"SDDM\")\n",
    "\n",
    "#sc.setMaster(\"local[*]\")#setMaster('spark://fs.das3.liacs.nl:7077')#(\"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# ==>> DO NOT FORGET WHNE YOU'RE DONE>> sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: SparkContext already exists in this scope\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# checking if a previous Spark session is active\n",
    "\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = SparkContext(appName=\"SDDM\", master='local[*]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SQLContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SparkSession (used for dataframes)\n",
    "\n",
    "from pyspark.sql import SparkSession #(or from pyspark.sql import SQLContext)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config(\"spark.driver.cores\", 1)\\\n",
    "        .appName(\"understanding_sparksession\")\\\n",
    "        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your application, please remember to call spark.stop()  in order to end the SparkSession. Let's understand the various settings that we define above:\n",
    "\n",
    "**master**: Sets the Spark master URL to connect to, such as “local” to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077” to run on a Spark standalone cluster.\n",
    "\n",
    "**config**:Sets a config option by specifying a (key, value) pair.\n",
    "\n",
    "**appName**: Sets a name for the application, if no name is set, a randomly generated name will be used.\n",
    "\n",
    "**getOrCreate**:Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder. In case an existing SparkSession is returned, the config options specified in this builder affecting the SQLContext configuration will applied. As SparkContext configuration cannot be modified on runtime (you have to stop existing context first), SQLContext configuration can be modified on runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Sometimes you need a full IDE to create more complex code, and PySpark isn’t on sys.path by default, \n",
    "but that doesn’t mean it can’t be used as a regular library. \n",
    "You can address this by adding PySpark to sys.path at runtime. \n",
    "The package findspark does that for you.\n",
    "\"\"\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n",
      "3.6\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "# Master can be local[*], spark:// , yarn, etc.\n",
    "# SparkContext available as sc, SQLContext available as sqlContext.\n",
    "\n",
    "# Inspecting SparkContext\n",
    "# Version: To retrieve SparkContext version\n",
    "\n",
    "print (sc.version)\n",
    "#2.3.1\n",
    "\n",
    "# Python Version: To retrieve Python version of SparkContext\n",
    "\n",
    "print(sc.pythonVer)\n",
    "#3.6\n",
    "\n",
    "# Master: URL of the cluster or “local” string to run in local mode of SparkContext\n",
    "\n",
    "print (sc.master)\n",
    "\n",
    "#local[*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark executon plan to show lazy evaluation with Word Count example\n",
    "\n",
    "# https://github.com/tirthajyoti/Spark-with-Python/blob/master/Word_Count.ipynb\n",
    "\n",
    "# SparkContext - number of workers and lazy evaluation¶\n",
    "\n",
    "# https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext_Workers_Lazy_Evaluations.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD (Resillient Distributed Datasets)\n",
    "\n",
    "### Basics of RDD\n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations.\n",
    "\n",
    "Formally, an RDD is a read-only, partitioned collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. \n",
    "\n",
    "Resilient: Ability to withstand failures\n",
    "\n",
    "Distributed: Spanning across multiple machines\n",
    "\n",
    "Datasets: Collection of partitioned data e.g, Arrays, Tables, Tuples etc.,\n",
    "\n",
    "RDD is \n",
    "- Lazily evaluated (transformations & actions)\n",
    "- Recomputed on node failure \n",
    "- Distributed across the cluster\n",
    "    \n",
    "Transformations (lazy) \n",
    "\n",
    "    map \n",
    "    filter \n",
    "    flatMap \n",
    "    reduceByKey \n",
    "    join \n",
    "    cogroup\n",
    "\n",
    "Actions (eager) \n",
    "\n",
    "    count \n",
    "    reduce \n",
    "    collect \n",
    "    take \n",
    "    saveAsTextFile \n",
    "    saveAsHadoop \n",
    "    countByValue\n",
    "\n",
    "\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "There are two ways to create RDDs,\n",
    "\n",
    "parallelizing an existing collection of objects in your driver program,\n",
    "\n",
    "External datasets (referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.)\n",
    "\n",
    "    Files in HDFS\n",
    "\n",
    "    Objects in Amazon S3 bucket\n",
    "\n",
    "    lines in a text file\n",
    "\n",
    "From existing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Loading data in PySpark\n",
    "\n",
    "# Parallelized collection (parallelizing)\n",
    "\n",
    "# parallelize() for creating RDDs from python lists\n",
    "\n",
    "numRDD = sc.paralle lize([1,2,3,4])\n",
    "\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "\n",
    "print (type(helloRDD))\n",
    "\n",
    "print (helloRDD.collect())\n",
    "\n",
    "#SparkContext's parallelize() method\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a text file and distributed it across worker nodes so that they can work on it in parallel. We could also parallelize lists using the function sc.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# creating RDDs from external datasets\n",
    "# SparkContext's textFile() method\n",
    "\n",
    "rdd2 = sc.textFile(\"example_text.txt\")\n",
    "\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use',\n",
       " 'it under the terms of the Project Gutenberg License included with this',\n",
       " 'eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: Ulysses',\n",
       " '',\n",
       " 'Author: James Joyce',\n",
       " '',\n",
       " 'Release Date: August 1, 2008 [EBook #4300]',\n",
       " 'Last Updated: August 17, 2017',\n",
       " '',\n",
       " 'Language: English',\n",
       " '',\n",
       " 'Character set encoding: UTF-8',\n",
       " '',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK ULYSSES ***',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Produced by Col Choat, and David Widger.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Ulysses',\n",
       " '',\n",
       " 'by James Joyce',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '— I —',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '[ 1 ]',\n",
       " '',\n",
       " 'Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of',\n",
       " 'lather on which a mirror and a razor lay crossed. A yellow dressinggown,',\n",
       " 'ungirdled, was sustained gently behind him on the mild morning air. He',\n",
       " 'held the bowl aloft and intoned:',\n",
       " '',\n",
       " '—Introibo ad altare Dei.',\n",
       " '',\n",
       " 'Halted, he peered down the dark winding stairs and called out coarsely:',\n",
       " '',\n",
       " '—Come up, Kinch! Come up, you fearful jesuit!',\n",
       " '',\n",
       " 'Solemnly he came forward and mounted the round gunrest. He faced about',\n",
       " 'and blessed gravely thrice the tower, the surrounding land and the',\n",
       " 'awaking mountains. Then, catching sight of Stephen Dedalus, he bent',\n",
       " 'towards him and made rapid crosses in the air, gurgling in his throat',\n",
       " 'and shaking his head. Stephen Dedalus, displeased and sleepy, leaned',\n",
       " 'his arms on the top of the staircase and looked coldly at the shaking',\n",
       " 'gurgling face that blessed him, equine in its length, and at the light',\n",
       " 'untonsured hair, grained and hued like pale oak.',\n",
       " '',\n",
       " 'Buck Mulligan peeped an instant under the mirror and then covered the',\n",
       " 'bowl smartly.',\n",
       " '',\n",
       " '—Back to barracks! he said sternly.',\n",
       " '',\n",
       " 'He added in a preacher’s tone:',\n",
       " '',\n",
       " '—For this, O dearly beloved, is the genuine Christine: body and soul',\n",
       " 'and blood and ouns. Slow music, please. Shut your eyes, gents. One',\n",
       " 'moment. A little trouble about those white corpuscles. Silence, all.',\n",
       " '',\n",
       " 'He peered sideways up and gave a long slow whistle of call, then paused',\n",
       " 'awhile in rapt attention, his even white teeth glistening here and there',\n",
       " 'with gold points. Chrysostomos. Two strong shrill whistles answered',\n",
       " 'through the calm.',\n",
       " '',\n",
       " '—Thanks, old chap, he cried briskly. That will do nicely. Switch off',\n",
       " 'the current, will you?',\n",
       " '',\n",
       " 'He skipped off the gunrest and looked gravely at his watcher, gathering',\n",
       " 'about his legs the loose folds of his gown. The plump shadowed face and',\n",
       " 'sullen oval jowl recalled a prelate, patron of arts in the middle ages.',\n",
       " 'A pleasant smile broke quietly over his lips.',\n",
       " '',\n",
       " '—The mockery of it! he said gaily. Your absurd name, an ancient Greek!',\n",
       " '',\n",
       " 'He pointed his finger in friendly jest and went over to the parapet,',\n",
       " 'laughing to himself. Stephen Dedalus stepped up, followed him wearily',\n",
       " 'halfway and sat down on the edge of the gunrest, watching him still as',\n",
       " 'he propped his mirror on the parapet, dipped the brush in the bowl and',\n",
       " 'lathered cheeks and neck.',\n",
       " '',\n",
       " 'Buck Mulligan’s gay voice went on.',\n",
       " '',\n",
       " '—My name is absurd too: Malachi Mulligan, two dactyls. But it has a',\n",
       " 'Hellenic ring, hasn’t it? Tripping and sunny like the buck himself.',\n",
       " 'We must go to Athens. Will you come if I can get the aunt to fork out',\n",
       " 'twenty quid?',\n",
       " '',\n",
       " 'He laid the brush aside and, laughing with delight, cried:',\n",
       " '',\n",
       " '—Will he come? The jejune jesuit!',\n",
       " '',\n",
       " 'Ceasing, he began to shave with care.',\n",
       " '',\n",
       " '—Tell me, Mulligan, Stephen said quietly.',\n",
       " '',\n",
       " '—Yes, my love?',\n",
       " '',\n",
       " '—How long is Haines going to stay in this tower?',\n",
       " '',\n",
       " 'Buck Mulligan showed a shaven cheek over his right shoulder.',\n",
       " '',\n",
       " '—God, isn’t he dreadful? he said frankly. A ponderous Saxon. He',\n",
       " 'thinks you’re not a gentleman. God, these bloody English! Bursting',\n",
       " 'with money and indigestion. Because he comes from Oxford. You know,',\n",
       " 'Dedalus, you have the real Oxford manner. He can’t make you out. O, my',\n",
       " 'name for you is the best: Kinch, the knife-blade.',\n",
       " '',\n",
       " 'He shaved warily over his chin.',\n",
       " '',\n",
       " '—He was raving all night about a black panther, Stephen said. Where is',\n",
       " 'his guncase?',\n",
       " '',\n",
       " '—A woful lunatic! Mulligan said. Were you in a funk?',\n",
       " '',\n",
       " '—I was, Stephen said with energy and growing fear. Out here in the',\n",
       " 'dark with a man I don’t know raving and moaning to himself about',\n",
       " 'shooting a black panther. You saved men from drowning. I’m not a hero,',\n",
       " 'however. If he stays on here I am off.',\n",
       " '',\n",
       " 'Buck Mulligan frowned at the lather on his razorblade. He hopped down',\n",
       " 'from his perch and began to search his trouser pockets hastily.',\n",
       " '',\n",
       " '—Scutter! he cried thickly.',\n",
       " '',\n",
       " 'He came over to the gunrest and, thrusting a hand into Stephen’s upper',\n",
       " 'pocket, said:',\n",
       " '',\n",
       " '—Lend us a loan of your noserag to wipe my razor.',\n",
       " '',\n",
       " 'Stephen suffered him to pull out and hold up on show by its corner a',\n",
       " 'dirty crumpled handkerchief. Buck Mulligan wiped the razorblade neatly.',\n",
       " 'Then, gazing over the handkerchief, he said:',\n",
       " '',\n",
       " '—The bard’s noserag! A new art colour for our Irish poets:',\n",
       " 'snotgreen. You can almost taste it, can’t you?',\n",
       " '',\n",
       " 'He mounted to the parapet again and gazed out over Dublin bay, his fair',\n",
       " 'oakpale hair stirring slightly.',\n",
       " '',\n",
       " '—God! he said quietly. Isn’t the sea what Algy calls it: a great',\n",
       " 'sweet mother? The snotgreen sea. The scrotumtightening sea. Epi oinopa',\n",
       " 'ponton. Ah, Dedalus, the Greeks! I must teach you. You must read them',\n",
       " 'in the original. Thalatta! Thalatta! She is our great sweet mother. Come',\n",
       " 'and look.',\n",
       " '',\n",
       " 'Stephen stood up and went over to the parapet. Leaning on it he looked',\n",
       " 'down on the water and on the mailboat clearing the harbourmouth of',\n",
       " 'Kingstown.',\n",
       " '',\n",
       " '—Our mighty mother! Buck Mulligan said.',\n",
       " '',\n",
       " 'He turned abruptly his grey searching eyes from the sea to Stephen’s',\n",
       " 'face.',\n",
       " '',\n",
       " '—The aunt thinks you killed your mother, he said. That’s why she',\n",
       " 'won’t let me have anything to do with you.',\n",
       " '',\n",
       " '—Someone killed her, Stephen said gloomily.',\n",
       " '',\n",
       " '—You could have knelt down, damn it, Kinch, when your dying mother',\n",
       " 'asked you, Buck Mulligan said. I’m hyperborean as much as you. But to',\n",
       " 'think of your mother begging you with her last breath to kneel down and',\n",
       " 'pray for her. And you refused. There is something sinister in you....',\n",
       " '',\n",
       " 'He broke off and lathered again lightly his farther cheek. A tolerant',\n",
       " 'smile curled his lips.',\n",
       " '',\n",
       " '—But a lovely mummer! he murmured to himself. Kinch, the loveliest',\n",
       " 'mummer of them all!',\n",
       " '',\n",
       " 'He shaved evenly and with care, in silence, seriously.',\n",
       " '',\n",
       " 'Stephen, an elbow rested on the jagged granite, leaned his palm against',\n",
       " 'his brow and gazed at the fraying edge of his shiny black coat-sleeve.',\n",
       " 'Pain, that was not yet the pain of love, fretted his heart. Silently, in',\n",
       " 'a dream she had come to him after her death, her wasted body within its',\n",
       " 'loose brown graveclothes giving off an odour of wax and rosewood, her',\n",
       " 'breath, that had bent upon him, mute, reproachful, a faint odour of',\n",
       " 'wetted ashes. Across the threadbare cuffedge he saw the sea hailed as a',\n",
       " 'great sweet mother by the wellfed voice beside him. The ring of bay',\n",
       " 'and skyline held a dull green mass of liquid. A bowl of white china had',\n",
       " 'stood beside her deathbed holding the green sluggish bile which she had',\n",
       " 'torn up from her rotting liver by fits of loud groaning vomiting.',\n",
       " '',\n",
       " 'Buck Mulligan wiped again his razorblade.',\n",
       " '',\n",
       " '—Ah, poor dogsbody! he said in a kind voice. I must give you a shirt',\n",
       " 'and a few noserags. How are the secondhand breeks?',\n",
       " '',\n",
       " '—They fit well enough, Stephen answered.',\n",
       " '',\n",
       " 'Buck Mulligan attacked the hollow beneath his underlip.',\n",
       " '',\n",
       " '—The mockery of it, he said contentedly. Secondleg they should be. God',\n",
       " 'knows what poxy bowsy left them off. I have a lovely pair with a hair',\n",
       " 'stripe, grey. You’ll look spiffing in them. I’m not joking, Kinch.',\n",
       " 'You look damn well when you’re dressed.',\n",
       " '',\n",
       " '—Thanks, Stephen said. I can’t wear them if they are grey.',\n",
       " '',\n",
       " '—He can’t wear them, Buck Mulligan told his face in the mirror.',\n",
       " 'Etiquette is etiquette. He kills his mother but he can’t wear grey',\n",
       " 'trousers.',\n",
       " '',\n",
       " 'He folded his razor neatly and with stroking palps of fingers felt the',\n",
       " 'smooth skin.',\n",
       " '',\n",
       " 'Stephen turned his gaze from the sea and to the plump face with its',\n",
       " 'smokeblue mobile eyes.',\n",
       " '',\n",
       " '—That fellow I was with in the Ship last night, said Buck Mulligan,',\n",
       " 'says you have g. p. i. He’s up in Dottyville with Connolly Norman.',\n",
       " 'General paralysis of the insane!',\n",
       " '',\n",
       " 'He swept the mirror a half circle in the air to flash the tidings abroad',\n",
       " 'in sunlight now radiant on the sea. His curling shaven lips laughed and',\n",
       " 'the edges of his white glittering teeth. Laughter seized all his strong',\n",
       " 'wellknit trunk.',\n",
       " '',\n",
       " '—Look at yourself, he said, you dreadful bard!',\n",
       " '',\n",
       " 'Stephen bent forward and peered at the mirror held out to him, cleft by',\n",
       " 'a crooked crack. Hair on end. As he and others see me. Who chose this',\n",
       " 'face for me? This dogsbody to rid of vermin. It asks me too.',\n",
       " '',\n",
       " '—I pinched it out of the skivvy’s room, Buck Mulligan said. It does',\n",
       " 'her all right. The aunt always keeps plainlooking servants for Malachi.',\n",
       " 'Lead him not into temptation. And her name is Ursula.',\n",
       " '',\n",
       " 'Laughing again, he brought the mirror away from Stephen’s peering',\n",
       " 'eyes.',\n",
       " '',\n",
       " '—The rage of Caliban at not seeing his face in a mirror, he said. If',\n",
       " 'Wilde were only alive to see you!',\n",
       " '',\n",
       " 'Drawing back and pointing, Stephen said with bitterness:',\n",
       " '',\n",
       " '—It is a symbol of Irish art. The cracked lookingglass of a servant.',\n",
       " '',\n",
       " 'Buck Mulligan suddenly linked his arm in Stephen’s and walked with him',\n",
       " 'round the tower, his razor and mirror clacking in the pocket where he',\n",
       " 'had thrust them.',\n",
       " '',\n",
       " '—It’s not fair to tease you like that, Kinch, is it? he said kindly.',\n",
       " 'God knows you have more spirit than any of them.',\n",
       " '',\n",
       " 'Parried again. He fears the lancet of my art as I fear that of his. The',\n",
       " 'cold steel pen.',\n",
       " '',\n",
       " '—Cracked lookingglass of a servant! Tell that to the oxy chap',\n",
       " 'downstairs and touch him for a guinea. He’s stinking with money and',\n",
       " 'thinks you’re not a gentleman. His old fellow made his tin by selling',\n",
       " 'jalap to Zulus or some bloody swindle or other. God, Kinch, if you and I',\n",
       " 'could only work together we might do something for the island. Hellenise',\n",
       " 'it.',\n",
       " '',\n",
       " 'Cranly’s arm. His arm.',\n",
       " '',\n",
       " '—And to think of your having to beg from these swine. I’m the only',\n",
       " 'one that knows what you are. Why don’t you trust me more? What have',\n",
       " 'you up your nose against me? Is it Haines? If he makes any noise here',\n",
       " 'I’ll bring down Seymour and we’ll give him a ragging worse than they',\n",
       " 'gave Clive Kempthorpe.',\n",
       " '',\n",
       " 'Young shouts of moneyed voices in Clive Kempthorpe’s rooms. Palefaces:',\n",
       " 'they hold their ribs with laughter, one clasping another. O, I shall',\n",
       " 'expire! Break the news to her gently, Aubrey! I shall die! With slit',\n",
       " 'ribbons of his shirt whipping the air he hops and hobbles round the',\n",
       " 'table, with trousers down at heels, chased by Ades of Magdalen with',\n",
       " 'the tailor’s shears. A scared calf’s face gilded with marmalade. I',\n",
       " 'don’t want to be debagged! Don’t you play the giddy ox with me!',\n",
       " '',\n",
       " 'Shouts from the open window startling evening in the quadrangle. A deaf',\n",
       " 'gardener, aproned, masked with Matthew Arnold’s face, pushes his mower',\n",
       " 'on the sombre lawn watching narrowly the dancing motes of grasshalms.',\n",
       " '',\n",
       " 'To ourselves... new paganism... omphalos.',\n",
       " '',\n",
       " '—Let him stay, Stephen said. There’s nothing wrong with him except',\n",
       " 'at night.',\n",
       " '',\n",
       " '—Then what is it? Buck Mulligan asked impatiently. Cough it up. I’m',\n",
       " 'quite frank with you. What have you against me now?',\n",
       " '',\n",
       " 'They halted, looking towards the blunt cape of Bray Head that lay on the',\n",
       " 'water like the snout of a sleeping whale. Stephen freed his arm quietly.',\n",
       " '',\n",
       " '—Do you wish me to tell you? he asked.',\n",
       " '',\n",
       " '—Yes, what is it? Buck Mulligan answered. I don’t remember anything.',\n",
       " '',\n",
       " 'He looked in Stephen’s face as he spoke. A light wind passed his brow,',\n",
       " 'fanning softly his fair uncombed hair and stirring silver points of',\n",
       " 'anxiety in his eyes.',\n",
       " '',\n",
       " 'Stephen, depressed by his own voice, said:',\n",
       " '',\n",
       " '—Do you remember the first day I went to your house after my',\n",
       " 'mother’s death?',\n",
       " '',\n",
       " 'Buck Mulligan frowned quickly and said:',\n",
       " '',\n",
       " '—What? Where? I can’t remember anything. I remember only ideas and',\n",
       " 'sensations. Why? What happened in the name of God?',\n",
       " '',\n",
       " '—You were making tea, Stephen said, and went across the landing to',\n",
       " 'get more hot water. Your mother and some visitor came out of the',\n",
       " 'drawingroom. She asked you who was in your room.',\n",
       " '',\n",
       " '—Yes? Buck Mulligan said. What did I say? I forget.',\n",
       " '',\n",
       " '—You said, Stephen answered, O, it’s only Dedalus whose mother is',\n",
       " 'beastly dead.',\n",
       " '',\n",
       " 'A flush which made him seem younger and more engaging rose to Buck',\n",
       " 'Mulligan’s cheek.',\n",
       " '',\n",
       " '—Did I say that? he asked. Well? What harm is that?',\n",
       " '',\n",
       " 'He shook his constraint from him nervously.',\n",
       " '',\n",
       " '—And what is death, he asked, your mother’s or yours or my own? You',\n",
       " 'saw only your mother die. I see them pop off every day in the Mater and',\n",
       " 'Richmond and cut up into tripes in the dissectingroom. It’s a beastly',\n",
       " 'thing and nothing else. It simply doesn’t matter. You wouldn’t kneel',\n",
       " 'down to pray for your mother on her deathbed when she asked you. Why?',\n",
       " 'Because you have the cursed jesuit strain in you, only it’s injected',\n",
       " 'the wrong way. To me it’s all a mockery and beastly. Her cerebral',\n",
       " 'lobes are not functioning. She calls the doctor sir Peter Teazle and',\n",
       " 'picks buttercups off the quilt. Humour her till it’s over. You crossed',\n",
       " 'her last wish in death and yet you sulk with me because I don’t whinge',\n",
       " 'like some hired mute from Lalouette’s. Absurd! I suppose I did say it.',\n",
       " 'I didn’t mean to offend the memory of your mother.',\n",
       " '',\n",
       " 'He had spoken himself into boldness. Stephen, shielding the gaping',\n",
       " 'wounds which the words had left in his heart, said very coldly:',\n",
       " '',\n",
       " '—I am not thinking of the offence to my mother.',\n",
       " '',\n",
       " '—Of what then? Buck Mulligan asked.',\n",
       " '',\n",
       " '—Of the offence to me, Stephen answered.',\n",
       " '',\n",
       " 'Buck Mulligan swung round on his heel.',\n",
       " '',\n",
       " '—O, an impossible person! he exclaimed.',\n",
       " '',\n",
       " 'He walked off quickly round the parapet. Stephen stood at his post,',\n",
       " 'gazing over the calm sea towards the headland. Sea and headland now grew',\n",
       " 'dim. Pulses were beating in his eyes, veiling their sight, and he felt',\n",
       " 'the fever of his cheeks.',\n",
       " '',\n",
       " 'A voice within the tower called loudly:',\n",
       " '',\n",
       " '—Are you up there, Mulligan?',\n",
       " '',\n",
       " '—I’m coming, Buck Mulligan answered.',\n",
       " '',\n",
       " 'He turned towards Stephen and said:',\n",
       " '',\n",
       " '—Look at the sea. What does it care about offences? Chuck Loyola,',\n",
       " 'Kinch, and come on down. The Sassenach wants his morning rashers.',\n",
       " '',\n",
       " 'His head halted again for a moment at the top of the staircase, level',\n",
       " 'with the roof:',\n",
       " '',\n",
       " '—Don’t mope over it all day, he said. I’m inconsequent. Give up',\n",
       " 'the moody brooding.',\n",
       " '',\n",
       " 'His head vanished but the drone of his descending voice boomed out of',\n",
       " 'the stairhead:',\n",
       " '',\n",
       " '     And no more turn aside and brood',\n",
       " '     Upon love’s bitter mystery',\n",
       " '     For Fergus rules the brazen cars.',\n",
       " 'Woodshadows floated silently by through the morning peace from the',\n",
       " 'stairhead seaward where he gazed. Inshore and farther out the mirror of',\n",
       " 'water whitened, spurned by lightshod hurrying feet. White breast of',\n",
       " 'the dim sea. The twining stresses, two by two. A hand plucking the',\n",
       " 'harpstrings, merging their twining chords. Wavewhite wedded words',\n",
       " 'shimmering on the dim tide.',\n",
       " '',\n",
       " 'A cloud began to cover the sun slowly, wholly, shadowing the bay in',\n",
       " 'deeper green. It lay beneath him, a bowl of bitter waters. Fergus’',\n",
       " 'song: I sang it alone in the house, holding down the long dark chords.',\n",
       " 'Her door was open: she wanted to hear my music. Silent with awe and pity',\n",
       " 'I went to her bedside. She was crying in her wretched bed. For those',\n",
       " 'words, Stephen: love’s bitter mystery.',\n",
       " '',\n",
       " 'Where now?',\n",
       " '',\n",
       " 'Her secrets: old featherfans, tasselled dancecards, powdered with musk,',\n",
       " 'a gaud of amber beads in her locked drawer. A birdcage hung in the sunny',\n",
       " 'window of her house when she was a girl. She heard old Royce sing in the',\n",
       " 'pantomime of Turko the Terrible and laughed with others when he sang:',\n",
       " '',\n",
       " '     I am the boy',\n",
       " '     That can enjoy',\n",
       " '     Invisibility.',\n",
       " 'Phantasmal mirth, folded away: muskperfumed.',\n",
       " '',\n",
       " 'And no more turn aside and brood.',\n",
       " '',\n",
       " 'Folded away in the memory of nature with her toys. Memories beset his',\n",
       " 'brooding brain. Her glass of water from the kitchen tap when she had',\n",
       " 'approached the sacrament. A cored apple, filled with brown sugar,',\n",
       " 'roasting for her at the hob on a dark autumn evening. Her shapely',\n",
       " 'fingernails reddened by the blood of squashed lice from the children’s',\n",
       " 'shirts.',\n",
       " '',\n",
       " 'In a dream, silently, she had come to him, her wasted body within its',\n",
       " 'loose graveclothes giving off an odour of wax and rosewood, her breath,',\n",
       " 'bent over him with mute secret words, a faint odour of wetted ashes.',\n",
       " '',\n",
       " 'Her glazing eyes, staring out of death, to shake and bend my soul. On me',\n",
       " 'alone. The ghostcandle to light her agony. Ghostly light on the tortured',\n",
       " 'face. Her hoarse loud breath rattling in horror, while all prayed on',\n",
       " 'their knees. Her eyes on me to strike me down. Liliata rutilantium te',\n",
       " 'confessorum turma circumdet: iubilantium te virginum chorus excipiat.',\n",
       " '',\n",
       " 'Ghoul! Chewer of corpses!',\n",
       " '',\n",
       " 'No, mother! Let me be and let me live.',\n",
       " '',\n",
       " '—Kinch ahoy!',\n",
       " '',\n",
       " 'Buck Mulligan’s voice sang from within the tower. It came nearer up',\n",
       " 'the staircase, calling again. Stephen, still trembling at his soul’s',\n",
       " 'cry, heard warm running sunlight and in the air behind him friendly',\n",
       " 'words.',\n",
       " '',\n",
       " '—Dedalus, come down, like a good mosey. Breakfast is ready. Haines is',\n",
       " 'apologising for waking us last night. It’s all right.',\n",
       " '',\n",
       " '—I’m coming, Stephen said, turning.',\n",
       " '',\n",
       " '—Do, for Jesus’ sake, Buck Mulligan said. For my sake and for all',\n",
       " 'our sakes.',\n",
       " '',\n",
       " 'His head disappeared and reappeared.',\n",
       " '',\n",
       " '—I told him your symbol of Irish art. He says it’s very clever.',\n",
       " 'Touch him for a quid, will you? A guinea, I mean.',\n",
       " '',\n",
       " '—I get paid this morning, Stephen said.',\n",
       " '',\n",
       " '—The school kip? Buck Mulligan said. How much? Four quid? Lend us one.',\n",
       " '',\n",
       " '—If you want it, Stephen said.',\n",
       " '',\n",
       " '—Four shining sovereigns, Buck Mulligan cried with delight. We’ll',\n",
       " 'have a glorious drunk to astonish the druidy druids. Four omnipotent',\n",
       " 'sovereigns.',\n",
       " '',\n",
       " 'He flung up his hands and tramped down the stone stairs, singing out of',\n",
       " 'tune with a Cockney accent:',\n",
       " '',\n",
       " '     O, won’t we have a merry time,',\n",
       " '     Drinking whisky, beer and wine!',\n",
       " '     On coronation,',\n",
       " '     Coronation day!',\n",
       " '     O, won’t we have a merry time',\n",
       " '     On coronation day!',\n",
       " 'Warm sunshine merrying over the sea. The nickel shavingbowl shone,',\n",
       " 'forgotten, on the parapet. Why should I bring it down? Or leave it there',\n",
       " 'all day, forgotten friendship?',\n",
       " '',\n",
       " 'He went over to it, held it in his hands awhile, feeling its coolness,',\n",
       " 'smelling the clammy slaver of the lather in which the brush was stuck.',\n",
       " 'So I carried the boat of incense then at Clongowes. I am another now and',\n",
       " 'yet the same. A servant too. A server of a servant.',\n",
       " '',\n",
       " 'In the gloomy domed livingroom of the tower Buck Mulligan’s gowned',\n",
       " 'form moved briskly to and fro about the hearth, hiding and revealing its',\n",
       " 'yellow glow. Two shafts of soft daylight fell across the flagged floor',\n",
       " 'from the high barbacans: and at the meeting of their rays a cloud of',\n",
       " 'coalsmoke and fumes of fried grease floated, turning.',\n",
       " '',\n",
       " '—We’ll be choked, Buck Mulligan said. Haines, open that door, will',\n",
       " 'you?',\n",
       " '',\n",
       " 'Stephen laid the shavingbowl on the locker. A tall figure rose from the',\n",
       " 'hammock where it had been sitting, went to the doorway and pulled open',\n",
       " 'the inner doors.',\n",
       " '',\n",
       " '—Have you the key? a voice asked.',\n",
       " '',\n",
       " '—Dedalus has it, Buck Mulligan said. Janey Mack, I’m choked!',\n",
       " '',\n",
       " 'He howled, without looking up from the fire:',\n",
       " '',\n",
       " '—Kinch!',\n",
       " '',\n",
       " '—It’s in the lock, Stephen said, coming forward.',\n",
       " '',\n",
       " 'The key scraped round harshly twice and, when the heavy door had been',\n",
       " 'set ajar, welcome light and bright air entered. Haines stood at the',\n",
       " 'doorway, looking out. Stephen haled his upended valise to the table and',\n",
       " 'sat down to wait. Buck Mulligan tossed the fry on to the dish beside',\n",
       " 'him. Then he carried the dish and a large teapot over to the table, set',\n",
       " 'them down heavily and sighed with relief.',\n",
       " '',\n",
       " '—I’m melting, he said, as the candle remarked when... But, hush!',\n",
       " 'Not a word more on that subject! Kinch, wake up! Bread, butter, honey.',\n",
       " 'Haines, come in. The grub is ready. Bless us, O Lord, and these thy',\n",
       " 'gifts. Where’s the sugar? O, jay, there’s no milk.',\n",
       " '',\n",
       " 'Stephen fetched the loaf and the pot of honey and the buttercooler from',\n",
       " 'the locker. Buck Mulligan sat down in a sudden pet.',\n",
       " '',\n",
       " '—What sort of a kip is this? he said. I told her to come after eight.',\n",
       " '',\n",
       " '—We can drink it black, Stephen said thirstily. There’s a lemon in',\n",
       " 'the locker.',\n",
       " '',\n",
       " '—O, damn you and your Paris fads! Buck Mulligan said. I want Sandycove',\n",
       " 'milk.',\n",
       " '',\n",
       " 'Haines came in from the doorway and said quietly:',\n",
       " '',\n",
       " '—That woman is coming up with the milk.',\n",
       " '',\n",
       " '—The blessings of God on you! Buck Mulligan cried, jumping up from his',\n",
       " 'chair. Sit down. Pour out the tea there. The sugar is in the bag. Here,',\n",
       " 'I can’t go fumbling at the damned eggs.',\n",
       " '',\n",
       " 'He hacked through the fry on the dish and slapped it out on three',\n",
       " 'plates, saying:',\n",
       " '',\n",
       " '—In nomine Patris et Filii et Spiritus Sancti.',\n",
       " '',\n",
       " 'Haines sat down to pour out the tea.',\n",
       " '',\n",
       " '—I’m giving you two lumps each, he said. But, I say, Mulligan, you',\n",
       " 'do make strong tea, don’t you?',\n",
       " '',\n",
       " 'Buck Mulligan, hewing thick slices from the loaf, said in an old',\n",
       " 'woman’s wheedling voice:',\n",
       " '',\n",
       " '—When I makes tea I makes tea, as old mother Grogan said. And when I',\n",
       " 'makes water I makes water.',\n",
       " '',\n",
       " '—By Jove, it is tea, Haines said.',\n",
       " '',\n",
       " 'Buck Mulligan went on hewing and wheedling:',\n",
       " '',\n",
       " '—So I do, Mrs Cahill, says she. Begob, ma’am, says Mrs Cahill, God',\n",
       " 'send you don’t make them in the one pot.',\n",
       " '',\n",
       " 'He lunged towards his messmates in turn a thick slice of bread, impaled',\n",
       " 'on his knife.',\n",
       " '',\n",
       " '—That’s folk, he said very earnestly, for your book, Haines. Five',\n",
       " 'lines of text and ten pages of notes about the folk and the fishgods of',\n",
       " 'Dundrum. Printed by the weird sisters in the year of the big wind.',\n",
       " '',\n",
       " 'He turned to Stephen and asked in a fine puzzled voice, lifting his',\n",
       " 'brows:',\n",
       " '',\n",
       " '—Can you recall, brother, is mother Grogan’s tea and water pot',\n",
       " 'spoken of in the Mabinogion or is it in the Upanishads?',\n",
       " '',\n",
       " '—I doubt it, said Stephen gravely.',\n",
       " '',\n",
       " '—Do you now? Buck Mulligan said in the same tone. Your reasons, pray?',\n",
       " '',\n",
       " '—I fancy, Stephen said as he ate, it did not exist in or out of the',\n",
       " 'Mabinogion. Mother Grogan was, one imagines, a kinswoman of Mary Ann.',\n",
       " '',\n",
       " 'Buck Mulligan’s face smiled with delight.',\n",
       " '',\n",
       " '—Charming! he said in a finical sweet voice, showing his white teeth',\n",
       " 'and blinking his eyes pleasantly. Do you think she was? Quite charming!',\n",
       " '',\n",
       " 'Then, suddenly overclouding all his features, he growled in a hoarsened',\n",
       " 'rasping voice as he hewed again vigorously at the loaf:',\n",
       " '',\n",
       " '     —For old Mary Ann',\n",
       " '     She doesn’t care a damn.',\n",
       " '     But, hising up her petticoats...',\n",
       " 'He crammed his mouth with fry and munched and droned.',\n",
       " '',\n",
       " 'The doorway was darkened by an entering form.',\n",
       " '',\n",
       " '—The milk, sir!',\n",
       " '',\n",
       " '—Come in, ma’am, Mulligan said. Kinch, get the jug.',\n",
       " '',\n",
       " 'An old woman came forward and stood by Stephen’s elbow.',\n",
       " '',\n",
       " '—That’s a lovely morning, sir, she said. Glory be to God.',\n",
       " '',\n",
       " '—To whom? Mulligan said, glancing at her. Ah, to be sure!',\n",
       " '',\n",
       " 'Stephen reached back and took the milkjug from the locker.',\n",
       " '',\n",
       " '—The islanders, Mulligan said to Haines casually, speak frequently of',\n",
       " 'the collector of prepuces.',\n",
       " '',\n",
       " '—How much, sir? asked the old woman.',\n",
       " '',\n",
       " '—A quart, Stephen said.',\n",
       " '',\n",
       " 'He watched her pour into the measure and thence into the jug rich white',\n",
       " 'milk, not hers. Old shrunken paps. She poured again a measureful and',\n",
       " 'a tilly. Old and secret she had entered from a morning world, maybe',\n",
       " 'a messenger. She praised the goodness of the milk, pouring it out.',\n",
       " 'Crouching by a patient cow at daybreak in the lush field, a witch on her',\n",
       " 'toadstool, her wrinkled fingers quick at the squirting dugs. They lowed',\n",
       " 'about her whom they knew, dewsilky cattle. Silk of the kine and poor old',\n",
       " 'woman, names given her in old times. A wandering crone, lowly form of',\n",
       " 'an immortal serving her conqueror and her gay betrayer, their common',\n",
       " 'cuckquean, a messenger from the secret morning. To serve or to upbraid,',\n",
       " 'whether he could not tell: but scorned to beg her favour.',\n",
       " '',\n",
       " '—It is indeed, ma’am, Buck Mulligan said, pouring milk into their',\n",
       " 'cups.',\n",
       " '',\n",
       " '—Taste it, sir, she said.',\n",
       " '',\n",
       " 'He drank at her bidding.',\n",
       " '',\n",
       " '—If we could live on good food like that, he said to her somewhat',\n",
       " 'loudly, we wouldn’t have the country full of rotten teeth and rotten',\n",
       " 'guts. Living in a bogswamp, eating cheap food and the streets paved with',\n",
       " 'dust, horsedung and consumptives’ spits.',\n",
       " '',\n",
       " '—Are you a medical student, sir? the old woman asked.',\n",
       " '',\n",
       " '—I am, ma’am, Buck Mulligan answered.',\n",
       " '',\n",
       " '—Look at that now, she said.',\n",
       " '',\n",
       " 'Stephen listened in scornful silence. She bows her old head to a voice',\n",
       " 'that speaks to her loudly, her bonesetter, her medicineman: me she',\n",
       " 'slights. To the voice that will shrive and oil for the grave all there',\n",
       " 'is of her but her woman’s unclean loins, of man’s flesh made not in',\n",
       " 'God’s likeness, the serpent’s prey. And to the loud voice that now',\n",
       " 'bids her be silent with wondering unsteady eyes.',\n",
       " '',\n",
       " '—Do you understand what he says? Stephen asked her.',\n",
       " '',\n",
       " '—Is it French you are talking, sir? the old woman said to Haines.',\n",
       " '',\n",
       " 'Haines spoke to her again a longer speech, confidently.',\n",
       " '',\n",
       " '—Irish, Buck Mulligan said. Is there Gaelic on you?',\n",
       " '',\n",
       " '—I thought it was Irish, she said, by the sound of it. Are you from',\n",
       " 'the west, sir?',\n",
       " '',\n",
       " '—I am an Englishman, Haines answered.',\n",
       " '',\n",
       " '—He’s English, Buck Mulligan said, and he thinks we ought to speak',\n",
       " 'Irish in Ireland.',\n",
       " '',\n",
       " '—Sure we ought to, the old woman said, and I’m ashamed I don’t',\n",
       " 'speak the language myself. I’m told it’s a grand language by them',\n",
       " 'that knows.',\n",
       " '',\n",
       " '—Grand is no name for it, said Buck Mulligan. Wonderful entirely. Fill',\n",
       " 'us out some more tea, Kinch. Would you like a cup, ma’am?',\n",
       " '',\n",
       " '—No, thank you, sir, the old woman said, slipping the ring of the',\n",
       " 'milkcan on her forearm and about to go.',\n",
       " '',\n",
       " 'Haines said to her:',\n",
       " '',\n",
       " '—Have you your bill? We had better pay her, Mulligan, hadn’t we?',\n",
       " '',\n",
       " 'Stephen filled again the three cups.',\n",
       " '',\n",
       " '—Bill, sir? she said, halting. Well, it’s seven mornings a pint at',\n",
       " 'twopence is seven twos is a shilling and twopence over and these three',\n",
       " 'mornings a quart at fourpence is three quarts is a shilling. That’s a',\n",
       " 'shilling and one and two is two and two, sir.',\n",
       " '',\n",
       " 'Buck Mulligan sighed and, having filled his mouth with a crust thickly',\n",
       " 'buttered on both sides, stretched forth his legs and began to search his',\n",
       " 'trouser pockets.',\n",
       " '',\n",
       " '—Pay up and look pleasant, Haines said to him, smiling.',\n",
       " '',\n",
       " 'Stephen filled a third cup, a spoonful of tea colouring faintly the',\n",
       " 'thick rich milk. Buck Mulligan brought up a florin, twisted it round in',\n",
       " 'his fingers and cried:',\n",
       " '',\n",
       " '—A miracle!',\n",
       " '',\n",
       " 'He passed it along the table towards the old woman, saying:',\n",
       " '',\n",
       " '—Ask nothing more of me, sweet. All I can give you I give.',\n",
       " '',\n",
       " 'Stephen laid the coin in her uneager hand.',\n",
       " '',\n",
       " '—We’ll owe twopence, he said.',\n",
       " '',\n",
       " '—Time enough, sir, she said, taking the coin. Time enough. Good',\n",
       " 'morning, sir.',\n",
       " '',\n",
       " 'She curtseyed and went out, followed by Buck Mulligan’s tender chant:',\n",
       " '',\n",
       " '     —Heart of my heart, were it more,',\n",
       " '     More would be laid at your feet.',\n",
       " 'He turned to Stephen and said:',\n",
       " '',\n",
       " '—Seriously, Dedalus. I’m stony. Hurry out to your school kip and',\n",
       " 'bring us back some money. Today the bards must drink and junket. Ireland',\n",
       " 'expects that every man this day will do his duty.',\n",
       " '',\n",
       " '—That reminds me, Haines said, rising, that I have to visit your',\n",
       " 'national library today.',\n",
       " '',\n",
       " '—Our swim first, Buck Mulligan said.',\n",
       " '',\n",
       " 'He turned to Stephen and asked blandly:',\n",
       " '',\n",
       " '—Is this the day for your monthly wash, Kinch?',\n",
       " '',\n",
       " 'Then he said to Haines:',\n",
       " '',\n",
       " '—The unclean bard makes a point of washing once a month.',\n",
       " '',\n",
       " '—All Ireland is washed by the gulfstream, Stephen said as he let honey',\n",
       " 'trickle over a slice of the loaf.',\n",
       " '',\n",
       " 'Haines from the corner where he was knotting easily a scarf about the',\n",
       " 'loose collar of his tennis shirt spoke:',\n",
       " '',\n",
       " '—I intend to make a collection of your sayings if you will let me.',\n",
       " '',\n",
       " 'Speaking to me. They wash and tub and scrub. Agenbite of inwit.',\n",
       " 'Conscience. Yet here’s a spot.',\n",
       " '',\n",
       " '—That one about the cracked lookingglass of a servant being the symbol',\n",
       " 'of Irish art is deuced good.',\n",
       " '',\n",
       " 'Buck Mulligan kicked Stephen’s foot under the table and said with',\n",
       " 'warmth of tone:',\n",
       " '',\n",
       " '—Wait till you hear him on Hamlet, Haines.',\n",
       " '',\n",
       " '—Well, I mean it, Haines said, still speaking to Stephen. I was just',\n",
       " 'thinking of it when that poor old creature came in.',\n",
       " '',\n",
       " '—Would I make any money by it? Stephen asked.',\n",
       " '',\n",
       " 'Haines laughed and, as he took his soft grey hat from the holdfast of',\n",
       " 'the hammock, said:',\n",
       " '',\n",
       " '—I don’t know, I’m sure.',\n",
       " '',\n",
       " 'He strolled out to the doorway. Buck Mulligan bent across to Stephen and',\n",
       " 'said with coarse vigour:',\n",
       " '',\n",
       " '—You put your hoof in it now. What did you say that for?',\n",
       " '',\n",
       " '—Well? Stephen said. The problem is to get money. From whom? From the',\n",
       " 'milkwoman or from him. It’s a toss up, I think.',\n",
       " '',\n",
       " '—I blow him out about you, Buck Mulligan said, and then you come along',\n",
       " 'with your lousy leer and your gloomy jesuit jibes.',\n",
       " '',\n",
       " '—I see little hope, Stephen said, from her or from him.',\n",
       " '',\n",
       " 'Buck Mulligan sighed tragically and laid his hand on Stephen’s arm.',\n",
       " '',\n",
       " '—From me, Kinch, he said.',\n",
       " '',\n",
       " 'In a suddenly changed tone he added:',\n",
       " '',\n",
       " '—To tell you the God’s truth I think you’re right. Damn all else',\n",
       " 'they are good for. Why don’t you play them as I do? To hell with them',\n",
       " 'all. Let us get out of the kip.',\n",
       " '',\n",
       " 'He stood up, gravely ungirdled and disrobed himself of his gown, saying',\n",
       " 'resignedly:',\n",
       " '',\n",
       " '—Mulligan is stripped of his garments.',\n",
       " '',\n",
       " 'He emptied his pockets on to the table.',\n",
       " '',\n",
       " '—There’s your snotrag, he said.',\n",
       " '',\n",
       " 'And putting on his stiff collar and rebellious tie he spoke to them,',\n",
       " 'chiding them, and to his dangling watchchain. His hands plunged and',\n",
       " 'rummaged in his trunk while he called for a clean handkerchief. God,',\n",
       " 'we’ll simply have to dress the character. I want puce gloves and',\n",
       " 'green boots. Contradiction. Do I contradict myself? Very well then, I',\n",
       " 'contradict myself. Mercurial Malachi. A limp black missile flew out of',\n",
       " 'his talking hands.',\n",
       " '',\n",
       " '—And there’s your Latin quarter hat, he said.',\n",
       " '',\n",
       " 'Stephen picked it up and put it on. Haines called to them from the',\n",
       " 'doorway:',\n",
       " '',\n",
       " '—Are you coming, you fellows?',\n",
       " '',\n",
       " '—I’m ready, Buck Mulligan answered, going towards the door. Come',\n",
       " 'out, Kinch. You have eaten all we left, I suppose. Resigned he passed',\n",
       " 'out with grave words and gait, saying, wellnigh with sorrow:',\n",
       " '',\n",
       " '—And going forth he met Butterly.',\n",
       " '',\n",
       " 'Stephen, taking his ashplant from its leaningplace, followed them out',\n",
       " 'and, as they went down the ladder, pulled to the slow iron door and',\n",
       " 'locked it. He put the huge key in his inner pocket.',\n",
       " '',\n",
       " 'At the foot of the ladder Buck Mulligan asked:',\n",
       " '',\n",
       " '—Did you bring the key?',\n",
       " '',\n",
       " '—I have it, Stephen said, preceding them.',\n",
       " '',\n",
       " 'He walked on. Behind him he heard Buck Mulligan club with his heavy',\n",
       " 'bathtowel the leader shoots of ferns or grasses.',\n",
       " '',\n",
       " '—Down, sir! How dare you, sir!',\n",
       " '',\n",
       " 'Haines asked:',\n",
       " '',\n",
       " '—Do you pay rent for this tower?',\n",
       " '',\n",
       " '—Twelve quid, Buck Mulligan said.',\n",
       " '',\n",
       " '—To the secretary of state for war, Stephen added over his shoulder.',\n",
       " '',\n",
       " 'They halted while Haines surveyed the tower and said at last:',\n",
       " '',\n",
       " '—Rather bleak in wintertime, I should say. Martello you call it?',\n",
       " '',\n",
       " '—Billy Pitt had them built, Buck Mulligan said, when the French were',\n",
       " 'on the sea. But ours is the omphalos.',\n",
       " '',\n",
       " '—What is your idea of Hamlet? Haines asked Stephen.',\n",
       " '',\n",
       " '—No, no, Buck Mulligan shouted in pain. I’m not equal to Thomas',\n",
       " 'Aquinas and the fiftyfive reasons he has made out to prop it up. Wait',\n",
       " 'till I have a few pints in me first.',\n",
       " '',\n",
       " 'He turned to Stephen, saying, as he pulled down neatly the peaks of his',\n",
       " 'primrose waistcoat:',\n",
       " '',\n",
       " '—You couldn’t manage it under three pints, Kinch, could you?',\n",
       " '',\n",
       " '—It has waited so long, Stephen said listlessly, it can wait longer.',\n",
       " '',\n",
       " '—You pique my curiosity, Haines said amiably. Is it some paradox?',\n",
       " '',\n",
       " '—Pooh! Buck Mulligan said. We have grown out of Wilde and paradoxes.',\n",
       " 'It’s quite simple. He proves by algebra that Hamlet’s grandson is',\n",
       " 'Shakespeare’s grandfather and that he himself is the ghost of his own',\n",
       " 'father.',\n",
       " '',\n",
       " '—What? Haines said, beginning to point at Stephen. He himself?',\n",
       " '',\n",
       " 'Buck Mulligan slung his towel stolewise round his neck and, bending in',\n",
       " 'loose laughter, said to Stephen’s ear:',\n",
       " '',\n",
       " '—O, shade of Kinch the elder! Japhet in search of a father!',\n",
       " '',\n",
       " '—We’re always tired in the morning, Stephen said to Haines. And it',\n",
       " 'is rather long to tell.',\n",
       " '',\n",
       " 'Buck Mulligan, walking forward again, raised his hands.',\n",
       " '',\n",
       " '—The sacred pint alone can unbind the tongue of Dedalus, he said.',\n",
       " '',\n",
       " '—I mean to say, Haines explained to Stephen as they followed, this',\n",
       " 'tower and these cliffs here remind me somehow of Elsinore. That beetles',\n",
       " 'o’er his base into the sea, isn’t it?',\n",
       " '',\n",
       " 'Buck Mulligan turned suddenly for an instant towards Stephen but did not',\n",
       " 'speak. In the bright silent instant Stephen saw his own image in cheap',\n",
       " 'dusty mourning between their gay attires.',\n",
       " '',\n",
       " '—It’s a wonderful tale, Haines said, bringing them to halt again.',\n",
       " '',\n",
       " 'Eyes, pale as the sea the wind had freshened, paler, firm and prudent.',\n",
       " 'The seas’ ruler, he gazed southward over the bay, empty save for',\n",
       " 'the smokeplume of the mailboat vague on the bright skyline and a sail',\n",
       " 'tacking by the Muglins.',\n",
       " '',\n",
       " '—I read a theological interpretation of it somewhere, he said bemused.',\n",
       " 'The Father and the Son idea. The Son striving to be atoned with the',\n",
       " 'Father.',\n",
       " '',\n",
       " 'Buck Mulligan at once put on a blithe broadly smiling face. He looked',\n",
       " 'at them, his wellshaped mouth open happily, his eyes, from which he had',\n",
       " 'suddenly withdrawn all shrewd sense, blinking with mad gaiety. He moved',\n",
       " 'a doll’s head to and fro, the brims of his Panama hat quivering, and',\n",
       " 'began to chant in a quiet happy foolish voice:',\n",
       " '',\n",
       " '     —I’m the queerest young fellow that ever you heard.',\n",
       " '     My mother’s a jew, my father’s a bird.',\n",
       " '     With Joseph the joiner I cannot agree.',\n",
       " '     So here’s to disciples and Calvary.',\n",
       " 'He held up a forefinger of warning.',\n",
       " '',\n",
       " '     —If anyone thinks that I amn’t divine',\n",
       " '     He’ll get no free drinks when I’m making the wine',\n",
       " '     But have to drink water and wish it were plain',\n",
       " '     That I make when the wine becomes water again.',\n",
       " 'He tugged swiftly at Stephen’s ashplant in farewell and, running',\n",
       " 'forward to a brow of the cliff, fluttered his hands at his sides like',\n",
       " 'fins or wings of one about to rise in the air, and chanted:',\n",
       " '',\n",
       " '     —Goodbye, now, goodbye! Write down all I said',\n",
       " '     And tell Tom, Dick and Harry I rose from the dead.',\n",
       " '     What’s bred in the bone cannot fail me to fly',\n",
       " '     And Olivet’s breezy... Goodbye, now, goodbye!',\n",
       " 'He capered before them down towards the fortyfoot hole, fluttering his',\n",
       " 'winglike hands, leaping nimbly, Mercury’s hat quivering in the fresh',\n",
       " 'wind that bore back to them his brief birdsweet cries.',\n",
       " '',\n",
       " 'Haines, who had been laughing guardedly, walked on beside Stephen and',\n",
       " 'said:',\n",
       " '',\n",
       " '—We oughtn’t to laugh, I suppose. He’s rather blasphemous. I’m',\n",
       " 'not a believer myself, that is to say. Still his gaiety takes the harm',\n",
       " 'out of it somehow, doesn’t it? What did he call it? Joseph the Joiner?',\n",
       " '',\n",
       " '—The ballad of joking Jesus, Stephen answered.',\n",
       " '',\n",
       " '—O, Haines said, you have heard it before?',\n",
       " '',\n",
       " '—Three times a day, after meals, Stephen said drily.',\n",
       " '',\n",
       " '—You’re not a believer, are you? Haines asked. I mean, a believer in',\n",
       " 'the narrow sense of the word. Creation from nothing and miracles and a',\n",
       " 'personal God.',\n",
       " '',\n",
       " '—There’s only one sense of the word, it seems to me, Stephen said.',\n",
       " '',\n",
       " 'Haines stopped to take out a smooth silver case in which twinkled a',\n",
       " 'green stone. He sprang it open with his thumb and offered it.',\n",
       " '',\n",
       " '—Thank you, Stephen said, taking a cigarette.',\n",
       " '',\n",
       " 'Haines helped himself and snapped the case to. He put it back in his',\n",
       " 'sidepocket and took from his waistcoatpocket a nickel tinderbox, sprang',\n",
       " 'it open too, and, having lit his cigarette, held the flaming spunk',\n",
       " 'towards Stephen in the shell of his hands.',\n",
       " '',\n",
       " '—Yes, of course, he said, as they went on again. Either you believe or',\n",
       " 'you don’t, isn’t it? Personally I couldn’t stomach that idea of a',\n",
       " 'personal God. You don’t stand for that, I suppose?',\n",
       " '',\n",
       " '—You behold in me, Stephen said with grim displeasure, a horrible',\n",
       " 'example of free thought.',\n",
       " '',\n",
       " 'He walked on, waiting to be spoken to, trailing his ashplant by his',\n",
       " 'side. Its ferrule followed lightly on the path, squealing at his heels.',\n",
       " 'My familiar, after me, calling, Steeeeeeeeeeeephen! A wavering line',\n",
       " 'along the path. They will walk on it tonight, coming here in the dark.',\n",
       " 'He wants that key. It is mine. I paid the rent. Now I eat his salt',\n",
       " 'bread. Give him the key too. All. He will ask for it. That was in his',\n",
       " 'eyes.',\n",
       " '',\n",
       " '—After all, Haines began...',\n",
       " '',\n",
       " 'Stephen turned and saw that the cold gaze which had measured him was not',\n",
       " 'all unkind.',\n",
       " '',\n",
       " '—After all, I should think you are able to free yourself. You are your',\n",
       " 'own master, it seems to me.',\n",
       " '',\n",
       " '—I am a servant of two masters, Stephen said, an English and an',\n",
       " 'Italian.',\n",
       " '',\n",
       " '—Italian? Haines said.',\n",
       " '',\n",
       " 'A crazy queen, old and jealous. Kneel down before me.',\n",
       " '',\n",
       " '—And a third, Stephen said, there is who wants me for odd jobs.',\n",
       " '',\n",
       " '—Italian? Haines said again. What do you mean?',\n",
       " '',\n",
       " '—The imperial British state, Stephen answered, his colour rising, and',\n",
       " 'the holy Roman catholic and apostolic church.',\n",
       " '',\n",
       " 'Haines detached from his underlip some fibres of tobacco before he',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add=rdd2.filter(lambda x: x!=\"\").collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use',\n",
       " 'it under the terms of the Project Gutenberg License included with this',\n",
       " 'eBook or online at www.gutenberg.org',\n",
       " 'Title: Ulysses',\n",
       " 'Author: James Joyce',\n",
       " 'Release Date: August 1, 2008 [EBook #4300]',\n",
       " 'Last Updated: August 17, 2017',\n",
       " 'Language: English']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32710"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32710\n",
      "[0, 54, 0, 71, 67]\n"
     ]
    }
   ],
   "source": [
    "LineLength = rdd2.map(lambda x : len(x))\n",
    "print (LineLength.count())\n",
    "print (LineLength.collect()[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Read Data from HDFS\n",
    "# hdfs://localhost:9746 filamentData.csv\n",
    "data = sc.textFile('hdfs://localhost:9746/bookData/filamentData.csv',4)\n",
    "data.take(4)\n",
    "\n",
    "# Read a file from HDFS and count the words\n",
    "https://github.com/radanalyticsio/radanalyticsio.github.io/blob/master/assets/pyspark_hdfs_notebook/PySpark_HDFS.ipynb\n",
    "\n",
    "# Reading input from S3 with Apache Spark on OpenShift\n",
    "https://github.com/radanalyticsio/radanalyticsio.github.io/blob/master/assets/s3-source-example/s3-source-example.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creates a DataFrame based on a table named \"people\"\n",
    "# stored in a MySQL database.\n",
    "url = \\\n",
    "  \"jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword\"\n",
    "df = sqlContext \\\n",
    "  .read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", url) \\\n",
    "  .option(\"dbtable\", \"people\") \\\n",
    "  .load()\n",
    "\n",
    "# Looks the schema of this DataFrame.\n",
    "df.printSchema()\n",
    "\n",
    "# Counts people by age\n",
    "countsByAge = df.groupBy(\"age\").count()\n",
    "countsByAge.show()\n",
    "\n",
    "# Saves countsByAge to S3 in the JSON format.\n",
    "countsByAge.write.format(\"json\").save(\"s3a://...\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save RDD Data to HDFS\n",
    "playData = sc.textFile('/home/muser/bData/shakespearePlays.txt',4)\n",
    "playDataLineLength = playData.map(lambda x : len(x))\n",
    "\n",
    "# Each file has a single data point because our RDD has four partitions.\n",
    "playDataLineLength.saveAsTextFile('hdfs://localhost:9746/savedData/')\n",
    "#  hadoop fs -cat /savedData/part-00000\n",
    "#  hadoop fs -cat /savedData/part-00001\n",
    "#  hadoop fs -cat /savedData/part-00002\n",
    "#  hadoop fs -cat /savedData/part-00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 's', 'r', 'p']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a CSV File\n",
    "# Writing a Python Function to Parse CSV Lines\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def parseCSV(csvRow) :\n",
    "    data = StringIO(csvRow)\n",
    "    dataReader = csv.reader(data, lineterminator = '')\n",
    "    return(next(dataReader))\n",
    "\n",
    "csvRow = \"p,s,r,p\"\n",
    "parseCSV(csvRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year',\n",
       "  'month',\n",
       "  'day',\n",
       "  'dep_time',\n",
       "  'dep_delay',\n",
       "  'arr_time',\n",
       "  'arr_delay',\n",
       "  'carrier',\n",
       "  'tailnum',\n",
       "  'flight',\n",
       "  'origin',\n",
       "  'dest',\n",
       "  'air_time',\n",
       "  'distance',\n",
       "  'hour',\n",
       "  'minute']]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file and Creating a Paired RDD\n",
    "filamentRDD = sc.textFile('flights_small.csv', 4)\n",
    "filamentRDDCSV = filamentRDD.map(parseCSV)\n",
    "filamentRDDCSV.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Understanding Partitioning in PySpark\n",
    "\n",
    "# A partition is a logical division of a large distributed data set\n",
    "\n",
    "# parallelize() method\n",
    "\n",
    "numRDD = sc.parallelize(range(10), numSlices = 3)\n",
    "\n",
    "#textFile() method\n",
    "\n",
    "fileRDD = sc.textFile(\"example_text.txt\", minPartitions = 6)\n",
    "\n",
    "#The number of partitions in an RDD can be found by using getNumPartitions() method\n",
    "\n",
    "print (fileRDD.getNumPartitions())\n",
    "\n",
    "print (numRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anonymous functions in Python\n",
    "\n",
    "Lambda functions are anonymous functions in Python\n",
    "\n",
    "Very powerful and used in Python. Quite efficient with map() and filter()\n",
    "\n",
    "Lambda functions create functions to be called later similar to def\n",
    "\n",
    "It returns the functions without any name (i.e anonymous)\n",
    "\n",
    "Inline a function definition or to defer execution of a code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transformations (lazy evaluation)\n",
    "\n",
    "# map() Transformation\n",
    "\n",
    "# map() transformation applies a function to all elements in the RDD\n",
    "\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter() Transformation\n",
    "\n",
    "# Filter transformation returns a new RDD with only the elements that pass the condition\n",
    "\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "\n",
    "RDD_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['how', 'are', 'you']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "\n",
    "RDD_str_map = RDD.map(lambda x: x.split(\" \"))\n",
    "\n",
    "RDD_str_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap() Transformation\n",
    "\n",
    "# flatMap() transformation returns multiple values for each element in the original RDD\n",
    "\n",
    "\"\"\"\n",
    "Why are we using flatMap, rather than map?\n",
    "\n",
    "The reason is that the operation line.split(\" \") generates a list of strings, \n",
    "so had we used map, the result would be an RDD of lists of words. Not an RDD of words.\n",
    "\n",
    "The difference between map and flatMap is that the second expects to get a list as the result \n",
    "from the map and it concatenates the lists to form the RDD.\n",
    "\"\"\"\n",
    "\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with money and indigestion. Because he comes from Oxford. You know,',\n",
       " 'downstairs and touch him for a guinea. He’s stinking with money and',\n",
       " '—Would I make any money by it? Stephen asked.',\n",
       " 'moved over the shells heaped in the cold stone mortar: whelks and money',\n",
       " '—Thank you, sir, Stephen said, gathering the money together with shy',\n",
       " 'don’t know yet what money is. Money is power. When you have lived',\n",
       " 'Shakespeare say? Put but money in thy purse.',\n",
       " '—He knew what money was, Mr Deasy said. He made money. A poet, yes,',\n",
       " 'of the canteen, over the motley slush. Even money Fair Rebel. Ten to one',\n",
       " 'twelve. By the way go easy with that money like a good young imbecile.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union() Transformation\n",
    "\n",
    "inputRDD = sc.textFile(\"example_text.txt\")\n",
    "\n",
    "money_RDD = inputRDD.filter(lambda x: \"money\" in x.split())\n",
    "biscuit_RDD = inputRDD.filter(lambda x: \"biscuit\" in x.split())\n",
    "combinedRDD =money_RDD.union(biscuit_RDD)\n",
    "\n",
    "combinedRDD.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with money and indigestion. Because he comes from Oxford. You know,',\n",
       " 'downstairs and touch him for a guinea. He’s stinking with money and',\n",
       " '—Would I make any money by it? Stephen asked.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD actions\n",
    "\n",
    "# Operation return a value after running a computation on the RDD\n",
    "\n",
    "# Basic RDD Actions: \n",
    "\n",
    "# collect () : collect() return all the elements of the dataset as an array\n",
    "\n",
    "# take() : take(N) returns an array with the first N elements of the dataset\n",
    "\n",
    "\n",
    "combinedRDD.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with money and indigestion. Because he comes from Oxford. You know,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first(), top() and count() Actions\n",
    "\n",
    "# first() prints the first element of the RDD\n",
    "\n",
    "combinedRDD.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So he went over to the biscuit tin Bob Doran left to see if there was'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedRDD.collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n"
     ]
    }
   ],
   "source": [
    "# Take top elements\n",
    "# This method should only be used if the resulting array is expected\n",
    "# to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "# It returns the list sorted in descending order.\n",
    "\n",
    "print (sc.parallelize([10, 4, 2, 12, 3]).top(1))\n",
    "\n",
    "#print (combinedRDD.top(2)) # the first two lines in a descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count() return the number of elements in the RDD\n",
    "\n",
    "combinedRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64]\n",
      "1\n",
      "8\n",
      "27\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "numbRDD = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect ()\n",
    "\n",
    "print (numbers_all)\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Number of partitions: 4\n",
      "Partitions structure: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# glom () - return an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "# https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0\n",
    "\n",
    "rdd=sc.parallelize(range(10), 4)\n",
    "\n",
    "print (rdd.collect())\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitions structure: [[0], [1], [2], [3, 4], [5], [6], [7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(10))\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to pair RDDs in PySpark\n",
    "\n",
    "Real life datasets are usually key/value pairs\n",
    "\n",
    "Each row is a key and maps to one or more values\n",
    "\n",
    "Pair RDD is a special data structure to work with this kind of datasets\n",
    "\n",
    "Pair RDD: Key is the identifier and value is data\n",
    "\n",
    "Creating pair RDDs\n",
    "\n",
    "Two common ways to create pair RDDs\n",
    "\n",
    "    From a list of key-value tuple\n",
    "    From a regular RDD\n",
    "    \n",
    "Get the data into key/value form for paired RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sam', 23), ('Mary', 34), ('Peter', 25)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Sam', '23'), ('Mary', '34'), ('Peter', '25')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "\n",
    "print (pairRDD_tuple.collect())\n",
    "\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "pairRDD_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pairRDD_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23', '34', '25']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fetching Values from a Paired RDD\n",
    "pairRDD_RDD_Values = pairRDD_RDD.values()\n",
    "pairRDD_RDD_Values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sam', 'Mary', 'Peter']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fetching Keys from a Paired RDD\n",
    "pairRDD_RDD_Keys = pairRDD_RDD.keys()\n",
    "pairRDD_RDD_Keys.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations on pair RDDs\n",
    "\n",
    "All regular transformations work on pair RDD\n",
    "\n",
    "Have to pass functions that operate on tuples rather than on individual elements\n",
    "\n",
    "Examples of paired RDD Transformations\n",
    "\n",
    "reduceByKey(func): Combine values with the same key\n",
    "\n",
    "groupByKey(): Group values with the same key\n",
    "\n",
    "sortByKey(): Return an RDD sorted by the key\n",
    "\n",
    "join(): Join two pair RDDs based on their key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 4, 25, 1]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use user functions to map on RDD\n",
    "\n",
    "def get_Squares(num):\n",
    "    return num**2\n",
    "\n",
    "numbRDD = sc.parallelize([1,2,3,4,2,5,1])\n",
    "\n",
    "numbRDD.map(get_Squares).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the distinct numbers\n",
    "\n",
    "numbRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Subract\n",
    "\n",
    "numbRDD2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "numbRDD.subtract(numbRDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  intersection\n",
    "\n",
    "numbRDD.intersection(numbRDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "10\n",
      "2.5\n",
      "1.25\n",
      "1.118033988749895\n",
      "(count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)\n",
      "{'count': 4, 'mean': 2.5, 'sum': 10.0, 'min': 1.0, 'max': 4.0, 'stdev': 1.2909944487358056, 'variance': 1.6666666666666667}\n"
     ]
    }
   ],
   "source": [
    "# calculating basic stats\n",
    "\n",
    "numbRDD = sc.parallelize([1,2,3,4,2,5,1])\n",
    "\n",
    "print (numRDD.min())\n",
    "\n",
    "print (numRDD.max())\n",
    "\n",
    "print (numRDD.sum())\n",
    "\n",
    "print (numRDD.mean())\n",
    "\n",
    "print (numRDD.variance())\n",
    "\n",
    "print (numRDD.stdev())\n",
    "\n",
    "print (numRDD.stats())\n",
    "\n",
    "print (numRDD.stats().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Messi', 23), ('Ronaldo', 34), ('Neymar', 22), ('Messi', 24)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Ronaldo', 34), ('Neymar', 22), ('Messi', 47)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey() transformation\n",
    "\n",
    "# reduceByKey() transformation combines values with the same key\n",
    "\n",
    "# It runs parallel operations for each key in the dataset\n",
    "\n",
    "# It is a transformation and not action\n",
    "\n",
    "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "\n",
    "print (regularRDD.collect())\n",
    "\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "\n",
    "pairRDD_reducebykey.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 22, 47]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD_reducebykey.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ronaldo', 'Neymar', 'Messi']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD_reducebykey.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey() transformation\n",
    "\n",
    "# sortByKey() operation orders pair RDD by key\n",
    "\n",
    "#It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('US', 'JFK'), ('UK', 'LHR'), ('FR', 'CDG'), ('US', 'SFO')]\n",
      "FR ['CDG'] 1\n",
      "UK ['LHR'] 1\n",
      "US ['JFK', 'SFO'] 2\n"
     ]
    }
   ],
   "source": [
    "# groupByKey() transformation\n",
    "\n",
    "# groupbykey() groups all the values with the same key in the pair RDD\n",
    "\n",
    "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\n",
    "\n",
    "regularRDD = sc.parallelize(airports)\n",
    "\n",
    "print (regularRDD.collect())\n",
    "\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air), len(air))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join() transformation\n",
    "\n",
    "# join() transformation joins the two pair RDDs based on their key\n",
    "\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "\n",
    "RDD1.join(RDD2).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce() action\n",
    "\n",
    "# reduce(func) action is used for aggregating the elements of a regular RDD\n",
    "\n",
    "# The function should be commutative and associative\n",
    "\n",
    "# An example of reduce() action in PySpark\n",
    "\n",
    "x = [1,3,4,6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 _SUCCESS\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 part-00000\n",
      "-rw-r--r--  1 vkocaman  staff  2 Oct 15 11:52 part-00001\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 part-00002\n",
      "-rw-r--r--  1 vkocaman  staff  2 Oct 15 11:52 part-00003\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 part-00004\n",
      "-rw-r--r--  1 vkocaman  staff  2 Oct 15 11:52 part-00005\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 part-00006\n",
      "-rw-r--r--  1 vkocaman  staff  2 Oct 15 11:52 part-00007\n"
     ]
    }
   ],
   "source": [
    "# saveAsTextFile() action\n",
    "# saveAsTextFile() action saves RDD into a text file inside a directory with each partition as a separate file\n",
    "\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "\n",
    "! cd tempFile && ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r--  1 vkocaman  staff  0 Oct 15 11:52 _SUCCESS\n",
      "-rw-r--r--  1 vkocaman  staff  8 Oct 15 11:52 part-00000\n"
     ]
    }
   ],
   "source": [
    "# coalesce() method can be used to save RDD as a single text file\n",
    "\n",
    "! rm -r tempFile # we remove the folder at first\n",
    "\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    "\n",
    "! cd tempFile && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Operations on pair RDDs\n",
    "\n",
    "RDD actions available for PySpark pair RDDs\n",
    "\n",
    "Pair RDD actions leverage the key-value data\n",
    "\n",
    "Few examples of pair RDD actions include\n",
    "\n",
    "- countByKey()\n",
    "\n",
    "- collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2\n",
      "b 1\n"
     ]
    }
   ],
   "source": [
    "# countByKey() action\n",
    "\n",
    "# countByKey() only available for type (K, V)\n",
    "\n",
    "# countByKey() action counts the number of elements for each key\n",
    "\n",
    "# Example of countByKey() on a simple list\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "for key, val in rdd.countByKey().items():\n",
    "    print(key, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectAsMap() action\n",
    "\n",
    "# collectAsMap() return the key-value pairs in the RDD as a dictionary\n",
    "\n",
    "# Example of collectAsMap() on a simple tuple\n",
    "\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13609, 'the'),\n",
       " (10549, ''),\n",
       " (8134, 'of'),\n",
       " (6551, 'and'),\n",
       " (5841, 'a'),\n",
       " (4788, 'to'),\n",
       " (4619, 'in'),\n",
       " (3034, 'his'),\n",
       " (2712, 'he'),\n",
       " (2430, 'I'),\n",
       " (2391, 'with'),\n",
       " (2169, 'that'),\n",
       " (2006, 'was'),\n",
       " (1894, 'on'),\n",
       " (1791, 'for'),\n",
       " (1680, 'it'),\n",
       " (1505, 'her'),\n",
       " (1363, 'you'),\n",
       " (1246, 'is'),\n",
       " (1217, 'at')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# “Hello World” of Distributed computing: The WordCount Program.\n",
    "\n",
    "text_file = sc.textFile(\"example_text.txt\")\n",
    "counts_rdd = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# print the word frequencies in descending order\n",
    "\n",
    "counts_rdd.map(lambda x: (x[1], x[0])) \\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .collect()[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate. So four times this fraction is equal to π.\n",
    "\n",
    "Note: If a circle of radius R is inscribed inside a square with side length 2R, then the area of the circle will be pi*R^2 and the area of the square will be (2R)^2. So the ratio of the area of the circle to the area of the square will be pi/4. This means that, if you pick N points at random inside the square, approximately N*pi/4 of those points should fall inside the circle.\n",
    "\n",
    "The \"Monte Carlo Method\" is a method of solving problems using statistics. Given the probability, P, that an event will occur in certain conditions, a computer can be used to generate those conditions repeatedly. The number of times the event occurs divided by the number of times the conditions are generated should be approximately equal to P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.135040\n"
     ]
    }
   ],
   "source": [
    "# how to find pi\n",
    "import random\n",
    "\n",
    "def inside_circle(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside_circle).count()\n",
    "\n",
    "print (\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams and word frequencies\n",
    "\n",
    "For a slightly more complicated task, lets look into splitting up sentences from our documents into word bigrams. A bigram is pair of successive tokens in some sequence. We will look at building bigrams from the sequences of words in each sentence, and then try to find the most frequently occuring ones.\n",
    "\n",
    "The first problem is that values in each partition of our initial RDD describe lines from the file rather than sentences. Sentences may be split over multiple lines. The glom() RDD method is used to create a single entry for each document containing the list of all lines, we can then join the lines up, then resplit them into sentences using \".\" as the separator, using flatMap so that every object in our RDD is now a sentence.\n",
    "\n",
    "Now we have isolated each sentence we can split it into a list of words and extract the word bigrams from it. Our new RDD contains tuples containing the word bigram (itself a tuple containing the first and second word) as the first value and the number 1 as the second value.\n",
    "\n",
    "Finally we can apply the same reduceByKey and sort steps that we used in the wordcount example, to count up the bigrams and sort them in order of descending frequency. In reduceByKey the key is not an individual word but a bigram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1635, ('of', 'the')),\n",
       " (1384, ('in', 'the')),\n",
       " (657, ('on', 'the')),\n",
       " (609, ('to', 'the')),\n",
       " (460, ('and', 'the')),\n",
       " (401, ('of', 'a')),\n",
       " (360, ('at', 'the')),\n",
       " (345, ('for', 'the')),\n",
       " (325, ('from', 'the')),\n",
       " (323, ('with', 'the'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams and word frequencies\n",
    "\n",
    "sentences = sc.textFile(\"example_text.txt\") \\\n",
    "    .glom() \\\n",
    "    .map(lambda x: \" \".join(x)) \\\n",
    "    .flatMap(lambda x: x.split(\".\"))\n",
    "\n",
    "bigrams = sentences.map(lambda x:x.split()) \\\n",
    "    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "\n",
    "freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False)\n",
    "\n",
    "freq_bigrams.take(10)\n",
    "\n",
    "# http://www.mccarroll.net/blog/pyspark2/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/Users/vkocaman/Python_Projects/Leiden-SDDM/Spark/example_text.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.wholeTextFiles(\"example_text.txt\", 8)\n",
    "\n",
    "rdd3.keys().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.values().collect()\n",
    "\n",
    "#textFile will return an RDD with each line as an element while wholeTextFiles returns a PairRDD with the key being the file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re used to working with Pandas or data frames in R, you’ll have probably also expected to see a header, but there is none. To make your life easier, you will move on from the RDD and convert it to a DataFrame. Dataframes are preferred over RDDs whenever you can use them. Especially when you’re working with Python, the performance of DataFrames is better than RDDs.\n",
    "\n",
    "But what is the difference between the two?\n",
    "\n",
    "You can use RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don’t necessarily want the performance benefits that DataFrames can offer for (semi-) structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
    "\n",
    "To recapitulate, you’ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. It is conceptually equivalent to a table in a relational database, an Excel sheet with Column headers, or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "It also shares some common characteristics with RDD:\n",
    "\n",
    "Immutable in nature : We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD after applying transformations.\n",
    "\n",
    "Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "\n",
    "Distributed: RDD and DataFrame both are distributed in nature.\n",
    "\n",
    "Advantages of the DataFrame:\n",
    "\n",
    "DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "\n",
    "Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "\n",
    "DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "\n",
    "DataFrame has a support for wide range of data format and sources.\n",
    "\n",
    "It has API support for different languages like Python, R, Scala, Java.\n",
    "\n",
    "\n",
    "### Using DataFrames\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark SQL is a Spark library for structured data. It provides more information about the structure of data and computation\n",
    "\n",
    "DataFrame is immutable distributed collection of data with named columns\n",
    "\n",
    "Designed for processing both structured (e.g relational database) and unstructured data (e.g JSON)\n",
    "\n",
    "Dataframe API is available in Java, Scala, Python, and R\n",
    "\n",
    "DataFrames in PySpark support both SQL queries (SELECT * from table) or expression methods (df.select())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparkSession - Entry point for DataFrame API\n",
    "\n",
    "SparkContext is the main entry point for creating RDDs\n",
    "\n",
    "SparkSession provides a single point of entry to interact with Spark DataFrames\n",
    "\n",
    "SparkSession is used to create DataFrame, register DataFrames, execute SQL queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.appName('SDDM_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames in PySpark\n",
    "\n",
    "Two different methods of creating DataFrames in PySpark\n",
    "\n",
    "From existing RDDs using SparkSession's createDataFrame() method\n",
    "\n",
    "From various data sources (CSV, JSON, TXT) using SparkSession's read method\n",
    "\n",
    "Schema controls the data and helps DataFrames to optimize queries\n",
    "\n",
    "Schema provides information about column name, type of data in the column, empty values etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from RDDD\n",
    "\n",
    "iphones_RDD = sc.parallelize([\n",
    "    (\"XS\", 2018, 5.65, 2.79, 6.24),\n",
    "    (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
    "    (\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "    (\"8Plus\", 2017, 6.23, 3.07, 7.12)\n",
    "])\n",
    "\n",
    "names = [ 'Model',\n",
    "          'Year',\n",
    "          'Height',\n",
    "          'Width',\n",
    "          'Weight'\n",
    "]\n",
    "\n",
    "iphones_df = ss.createDataFrame(iphones_RDD, schema=names)\n",
    "\n",
    "type(iphones_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-----+------+\n",
      "|Model|Year|Height|Width|Weight|\n",
      "+-----+----+------+-----+------+\n",
      "|   XS|2018|  5.65| 2.79|  6.24|\n",
      "|   XR|2018|  5.94| 2.98|  6.84|\n",
      "|  X10|2017|  5.65| 2.79|  6.13|\n",
      "+-----+----+------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iphones_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iphones_df.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Year</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XS</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.79</td>\n",
       "      <td>6.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XR</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.94</td>\n",
       "      <td>2.98</td>\n",
       "      <td>6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X10</td>\n",
       "      <td>2017</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.79</td>\n",
       "      <td>6.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8Plus</td>\n",
       "      <td>2017</td>\n",
       "      <td>6.23</td>\n",
       "      <td>3.07</td>\n",
       "      <td>7.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  Year  Height  Width  Weight\n",
       "0     XS  2018    5.65   2.79    6.24\n",
       "1     XR  2018    5.94   2.98    6.84\n",
       "2    X10  2017    5.65   2.79    6.13\n",
       "3  8Plus  2017    6.23   3.07    7.12"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphones_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wget -q -O - https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2017-06.csv | head -n 5000 > tmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from reading a CSV/JSON/TXT\n",
    "\n",
    "df_csv = ss.read.csv(\"people.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "df_json = ss.read.json(\"people.json\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "df_txt = ss.read.txt(\"people.txt\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# Path to the file and two optional parameters\n",
    "\n",
    "\n",
    "file_location = \"/FileStore/tables/game_skater_stats.csv\"\n",
    "\n",
    "df = ss.read.format(\"csv\").option(\"inferSchema\", \n",
    "           True).option(\"header\", True).load(file_location)\n",
    "\n",
    "# Two optional parameters\n",
    "\n",
    "# header=True and inferSchema=True\n",
    "\n",
    "# for more information aboput df.persist(StorageLevel.MEMORY_AND_DISK_SER) see below\n",
    "# https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading CSV files into dataframes, Spark performs the operation in an eager mode, meaning that all of the data is loaded into memory before the next step begins execution, while a lazy approach is used when reading files in the parquet format. Generally, you want to avoid eager operations when working with Spark, and if I need to process large CSV files I’ll first transform the data set to parquet format before executing the rest of the pipeline.\n",
    "Often you’ll need to process a large number of files, such as hundreds of parquet files located at a certain path or directory in DBFS. With Spark, you can include a wildcard in a path to process a collection of files. For example, you can load a batch of parquet files from S3 as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"s3a://my_bucket/game_skater_stats/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to reading data with Spark, it’s not recommended to write data to local storage when using PySpark. Instead, you should used a distributed file system such as S3 or HDFS. If you going to be processing the results with Spark, then parquet is a good format to use for saving data frames. The snippet below shows how to save a dataframe to DBFS and S3 as parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBFS (Parquet)\n",
    "df.write.save('/FileStore/parquet/game_stats',format='parquet')\n",
    "\n",
    "# S3 (Parquet)\n",
    "df.write.parquet(\"s3a://my_bucket/game_stats\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the results in a CSV file, then a slightly different output step is required. One of the main differences in this approach is that all of the data will be pulled to a single node before being output to CSV. This approach is recommended when you need to save a small dataframe and process it in a system outside of Spark. The snippet below shows how to save a dataframe as a single CSV file on DBFS and S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBFS (CSV)\n",
    "df.write.save('/FileStore/parquet/game_stats.csv', format='csv')\n",
    "\n",
    "# S3 (CSV)\n",
    "df.coalesce(1).write.format(\"com.databricks.spark.csv\")\n",
    "   .option(\"header\", \"true\").save(\"s3a://my_bucket/game_sstats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- faa: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- alt: integer (nullable = true)\n",
      " |-- tz: integer (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = ss.read.csv(\"airports.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# printSchema() operation prints the types of columns in the DataFrame\n",
    "\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame operators in PySpark\n",
    "\n",
    "DataFrame operations: Transformations and Actions\n",
    "\n",
    "DataFrame Transformations:\n",
    "\n",
    "select(), filter(), groupby(), orderby(), dropDuplicates() and withColumnRenamed()\n",
    "\n",
    "DataFrame Actions :\n",
    "\n",
    "printSchema(), head(), show(), count(), columns() and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport| 41.431912|-74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|31.0744722|-81.4277778|  11| -4|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|   Lansdowne Airport|\n",
      "|Moton Field Munic...|\n",
      "| Schaumburg Regional|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() and show() operations\n",
    "\n",
    "# select() transformation subsets the columns in the DataFrame\n",
    "\n",
    "df_id_name = df_csv.select('name')\n",
    "\n",
    "# show() action prints first 20 rows in the DataFrame\n",
    "\n",
    "df_id_name.show(3)\n",
    "\n",
    "# only showing top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|faa|                name|       lat|         lon|alt| tz|dst|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|09J|Jekyll Island Air...|31.0744722| -81.4277778| 11| -4|  A|\n",
      "|1RL|Point Roberts Air...|48.9797222|-123.0788889| 10| -7|  A|\n",
      "|369|  Atmautluak Airport| 60.866667| -162.273056| 18|-10|  A|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter() transformation filters out the rows based on a condition\n",
    "\n",
    "new_df = df_csv.filter(df_csv.alt < 100)\n",
    "\n",
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|faa|                name|       lat|         lon|alt| tz|dst|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "|09J|Jekyll Island Air...|31.0744722| -81.4277778| 11| -4|  A|\n",
      "|1RL|Point Roberts Air...|48.9797222|-123.0788889| 10| -7|  A|\n",
      "|369|  Atmautluak Airport| 60.866667| -162.273056| 18|-10|  A|\n",
      "+---+--------------------+----------+------------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also use brackets (as in Pandas) instead of filter()\n",
    "\n",
    "df_csv[df_csv.alt < 100].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "|faa|                name|       lat|        lon|alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "|60J|Ocean Isle Beach ...|33.9085056|-78.4366722| 32| -5|  U|\n",
      "|HHH|         Hilton Head|    32.216|    -80.752| 10| -5|  U|\n",
      "|HNL|       Honolulu Intl| 21.318681|-157.922428| 13|-10|  N|\n",
      "+---+--------------------+----------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_csv[(df_csv.alt < 100) & (df_csv[\"dst\"] != 'A')].show(3)\n",
    "\n",
    "# df_csv[(df_csv.alt < 100) & (df_csv.dst != 'A')].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dst|count|\n",
      "+---+-----+\n",
      "|  U|   45|\n",
      "|  A| 1329|\n",
      "|  N|   23|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby() operation can be used to group a variable\n",
    "\n",
    "df_csv_group = df_csv.groupby('dst')\n",
    "\n",
    "df_csv_group.count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dst|count|\n",
      "+---+-----+\n",
      "|  A| 1329|\n",
      "|  N|   23|\n",
      "|  U|   45|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderby() operation sorts the DataFrame based one or more columns\n",
    "\n",
    "df_csv_group.count().orderBy('dst').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropDuplicates() removes the duplicate rows of a DataFrame\n",
    "\n",
    "df_no_dup = df_csv.select('alt', 'dst').dropDuplicates()\n",
    "\n",
    "df_no_dup.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "|faa|                name|       lat|        lon|altitude| tz|dst|\n",
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|    1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278|     264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428|     801| -6|  A|\n",
      "+---+--------------------+----------+-----------+--------+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed() renames a column in the DataFrame\n",
    "\n",
    "df_csv_alt = df_csv.withColumnRenamed('alt', 'altitude')\n",
    "\n",
    "df_csv_alt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column from existing ones\n",
    "\n",
    "df_csv_alt = df_csv_alt.withColumn('tzxaltitude', df_csv_alt['tz'] * df_csv_alt['altitude'])\n",
    "# we didn't run this cell before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_csv_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+--------+---+-----------+\n",
      "|faa|                name|       lat|        lon|altitude|dst|tzxaltitude|\n",
      "+---+--------------------+----------+-----------+--------+---+-----------+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|    1044|  A|      -5220|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278|     264|  A|      -1320|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428|     801|  A|      -4806|\n",
      "+---+--------------------+----------+-----------+--------+---+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping a column (no axis concept)\n",
    "\n",
    "df_csv_alt= df_csv_alt.drop(\"tz\")\n",
    "\n",
    "df_csv_alt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faa', 'name', 'lat', 'lon', 'alt', 'tz', 'dst']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns() operator prints the columns of a DataFrame\n",
    "\n",
    "df_csv.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               lat|               lon|               alt|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1397|              1397|              1397|\n",
      "|   mean| 41.75029635989892|-103.6891285724532|1005.9169649248389|\n",
      "| stddev|10.549872185047212|30.125313702028542|1521.2701426664623|\n",
      "|    min|         19.721375|          -176.646|               -54|\n",
      "|    max|         72.270833|         174.11362|              9078|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe() operation compute summary statistics of numerical columns in the DataFrame\n",
    "\n",
    "df_csv.select('lat', 'lon', 'alt').describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with DataFrames using PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame API vs SQL queries\n",
    "\n",
    "In PySpark You can interact with SparkSQL through DataFrame API and SQL queries\n",
    "\n",
    "The DataFrame API provides a programmatic domain-specific language (DSL) for data\n",
    "\n",
    "DataFrame transformations and actions are easier to construct programmatically\n",
    "\n",
    "SQL queries can be concise and easier to understand and portable\n",
    "\n",
    "The operations on DataFrames can also be done using SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL notes\n",
    "\n",
    "A SQL query returns a table derived from one or more tables contained in a database.\n",
    "\n",
    "Every SQL query is made up of commands that tell the database what you want to do with the data. The two commands that every query has to contain are SELECT and FROM.\n",
    "\n",
    "The SELECT command is followed by the columns you want in the resulting table.\n",
    "\n",
    "The FROM command is followed by the name of the table that contains those columns. The minimal SQL query is:\n",
    "\n",
    "SELECT * FROM my_table;\n",
    "\n",
    "The * selects all columns, so this returns the entire table named my_table.\n",
    "\n",
    "Similar to .withColumn(), you can do column-wise computations within a SELECT statement. For example,\n",
    "\n",
    "SELECT origin, dest, air_time / 60 FROM flights;\n",
    "\n",
    "returns a table with the origin, destination, and duration in hours for each flight.\n",
    "\n",
    "Another commonly used command is WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where your condition is true. For example, if you had a table of students and grades you could do:\n",
    "\n",
    "SELECT * FROM students\n",
    "WHERE grade = 'A';\n",
    "\n",
    "to select all the columns and the rows containing information about students who got As.\n",
    "\n",
    "\n",
    "Another common database task is aggregation. That is, reducing your data by breaking it into chunks and summarizing each chunk.\n",
    "\n",
    "This is done in SQL using the GROUP BY command. This command breaks your data into groups and applies a function from your SELECT statement to each group.\n",
    "\n",
    "For example, if you wanted to count the number of flights from each of two origin destinations, you could use the query\n",
    "\n",
    "SELECT COUNT(*) FROM flights\n",
    "GROUP BY origin;\n",
    "\n",
    "GROUP BY origin tells SQL that you want the output to have a row for each unique value of the origin column. The SELECT statement selects the values you want to populate each of the columns. Here, we want to COUNT() every row in each of the groups.\n",
    "\n",
    "It's possible to GROUP BY more than one column. When you do this, the resulting table has a row for every combination of the unique values in each column. The following query counts the number of flights from SEA and PDX to every destination airport:\n",
    "\n",
    "SELECT origin, dest, COUNT(*) FROM flights\n",
    "GROUP BY origin, dest;\n",
    "\n",
    "The output will have a row for every combination of the values in origin and dest (i.e. a row listing each origin and destination that a flight flew to). There will also be a column with the COUNT() of all the rows in each group.\n",
    "\n",
    "Another very common data operation is the join. Joins are a whole topic unto themselves, so in this course we'll just look at simple joins. If you'd like to learn more about joins, you can take a look here.\n",
    "\n",
    "A join will combine two different tables along a column that they share. This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\n",
    "\n",
    "For example, suppose that you want to know more information about the plane that flew a flight than just the tail number. This information isn't in the flights table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, you'd have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table planes\n",
    "\n",
    "When you join the flights table to this table of airplane information, you're adding all the columns from the planes table to the flights table. To fill these columns with information, you'll look at the tail number from the flights table and find the matching one in the planes table, and then use that row to fill out all the new columns.\n",
    "\n",
    "Now you'll have a much bigger table than before, but now every row has all information about the plane that flew that flight!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+---+----+\n",
      "|                name|       lat|        lon|dst| alt|\n",
      "+--------------------+----------+-----------+---+----+\n",
      "|   Lansdowne Airport|41.1304722|-80.6195833|  A|1044|\n",
      "|Moton Field Munic...|32.4605722|-85.6800278|  A| 264|\n",
      "| Schaumburg Regional|41.9893408|-88.1012428|  A| 801|\n",
      "|     Randall Airport| 41.431912|-74.3915611|  A| 523|\n",
      "|Elizabethton Muni...|36.3712222|-82.1734167|  A|1593|\n",
      "+--------------------+----------+-----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing SQL Queries\n",
    "\n",
    "# The SparkSession sql() method executes SQL query\n",
    "\n",
    "# sql() method takes a SQL statement as an argument and returns the result as DataFrame\n",
    "\n",
    "df_csv.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "df2 = ss.sql(\"SELECT name, lat, lon, dst, alt FROM table1 WHERE alt > 100\")\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'lat', 'lon', 'dst', 'alt']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|dst|max(alt)|\n",
      "+---+--------+\n",
      "|  U|    6548|\n",
      "|  A|    9078|\n",
      "|  N|    7015|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarizing and grouping data using SQL queries\n",
    "\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "\n",
    "query = 'SELECT dst, max(alt) FROM table2 GROUP BY dst'\n",
    "\n",
    "ss.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table1', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='table2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = ss.read.csv (\"flights_small.csv\",inferSchema=True, header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2014, month=12, day=8, dep_time='658', dep_delay='-7', arr_time='935', arr_delay='-5', carrier='VX', tailnum='N846VA', flight=1780, origin='SEA', dest='LAX', air_time='132', distance=954, hour='6', minute='58'),\n",
       " Row(year=2014, month=1, day=22, dep_time='1040', dep_delay='5', arr_time='1505', arr_delay='5', carrier='AS', tailnum='N559AS', flight=851, origin='SEA', dest='HNL', air_time='360', distance=2677, hour='10', minute='40'),\n",
       " Row(year=2014, month=3, day=9, dep_time='1443', dep_delay='-2', arr_time='1652', arr_delay='2', carrier='VX', tailnum='N847VA', flight=755, origin='SEA', dest='SFO', air_time='111', distance=679, hour='14', minute='43'),\n",
       " Row(year=2014, month=4, day=9, dep_time='1705', dep_delay='45', arr_time='1839', arr_delay='34', carrier='WN', tailnum='N360SW', flight=344, origin='PDX', dest='SJC', air_time='83', distance=569, hour='17', minute='5'),\n",
       " Row(year=2014, month=3, day=9, dep_time='754', dep_delay='-1', arr_time='1015', arr_delay='1', carrier='AS', tailnum='N612AS', flight=522, origin='SEA', dest='BUR', air_time='127', distance=937, hour='7', minute='54'),\n",
       " Row(year=2014, month=1, day=15, dep_time='1037', dep_delay='7', arr_time='1352', arr_delay='2', carrier='WN', tailnum='N646SW', flight=48, origin='PDX', dest='DEN', air_time='121', distance=991, hour='10', minute='37'),\n",
       " Row(year=2014, month=7, day=2, dep_time='847', dep_delay='42', arr_time='1041', arr_delay='51', carrier='WN', tailnum='N422WN', flight=1520, origin='PDX', dest='OAK', air_time='90', distance=543, hour='8', minute='47'),\n",
       " Row(year=2014, month=5, day=12, dep_time='1655', dep_delay='-5', arr_time='1842', arr_delay='-18', carrier='VX', tailnum='N361VA', flight=755, origin='SEA', dest='SFO', air_time='98', distance=679, hour='16', minute='55'),\n",
       " Row(year=2014, month=4, day=19, dep_time='1236', dep_delay='-4', arr_time='1508', arr_delay='-7', carrier='AS', tailnum='N309AS', flight=490, origin='SEA', dest='SAN', air_time='135', distance=1050, hour='12', minute='36'),\n",
       " Row(year=2014, month=11, day=19, dep_time='1812', dep_delay='-3', arr_time='2352', arr_delay='-4', carrier='AS', tailnum='N564AS', flight=26, origin='SEA', dest='ORD', air_time='198', distance=1721, hour='18', minute='12')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = ss.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandafy a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  origin dest    N\n",
      "0    SEA  RNO    8\n",
      "1    SEA  DTW   98\n",
      "2    SEA  CLE    2\n",
      "3    SEA  LAX  450\n",
      "4    PDX  SEA  144\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = ss.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put some Spark in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = ss.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add spark_temp to the catalog\n",
    "spark_temp.name = spark_temp.createOrReplaceTempView('temp')\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "ss.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = ss.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/Users/vkocaman/Python_Projects/Leiden-SDDM/Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = ss.read.csv('flights_small.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'tailnum',\n",
       " 'flight',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|               2.2|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|              1.85|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|1.3833333333333333|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|2.1166666666666667|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add new column\n",
    "\n",
    "flights = flights.withColumn('duration_hrs', flights.air_time / 60)\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Filter flights with a SQL string\n",
    "long_flights1 = flights.filter('distance > 1000')\n",
    "\n",
    "# Filter flights with a boolean column\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "\n",
    "# Examine the data to check they're equal\n",
    "print(long_flights1.show(3))\n",
    "print(long_flights2.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| LAX|     VX|\n",
      "|   SEA| HNL|     AS|\n",
      "|   SEA| SFO|     VX|\n",
      "|   PDX| SJC|     WN|\n",
      "|   SEA| BUR|     AS|\n",
      "|   PDX| DEN|     WN|\n",
      "|   PDX| OAK|     WN|\n",
      "|   SEA| SFO|     VX|\n",
      "|   SEA| SAN|     AS|\n",
      "|   SEA| ORD|     AS|\n",
      "+------+----+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "+------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\",\"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "\n",
    "print (temp.show(10))\n",
    "\n",
    "# Define first filter\n",
    "filterA = (flights.origin == \"SEA\")\n",
    "\n",
    "# Define second filter\n",
    "filterB = (flights.dest == \"PDX\")\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)\n",
    "\n",
    "selected2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "|   PDX| SJC| N360SW| 411.3253012048193|\n",
      "|   SEA| BUR| N612AS| 442.6771653543307|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "print (speed1.show(5))\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
    "\n",
    "print (speed2.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting\n",
    "\n",
    "flights = flights.withColumn(\"distance\", flights.distance.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.withColumn(\"air_time\", flights.air_time.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.withColumn(\"dep_delay\", flights.dep_delay.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|          air_time|         distance|         dep_delay|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|              9925|            10000|              9952|\n",
      "|   mean|152.88423173803525|        1208.1516| 6.068629421221865|\n",
      "| stddev|  72.8656286392139|656.8599023464376|28.808608062751805|\n",
      "|    min|              20.0|             93.0|             -19.0|\n",
      "|    max|             409.0|           2724.0|             886.0|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.describe('air_time', 'distance', \"dep_delay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|        106.0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        409.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of duration\n",
    "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|188.20689655172413|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|25289.600000000126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of Delta flights\n",
    "flights.filter(flights.carrier == \"DL\")\\\n",
    "       .filter(flights.origin == \"SEA\")\\\n",
    "       .groupBy().avg('air_time')\\\n",
    "       .show()\n",
    "\n",
    "# Total hours in the air\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|               2.2|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|              1.85|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|1.3833333333333333|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|2.1166666666666667|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|2.0166666666666666|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|               1.5|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|1.6333333333333333|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|2.1666666666666665|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|2.1166666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|              2.15|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|               1.5|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|1.2666666666666666|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|              1.85|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N442AS|   38|\n",
      "| N102UW|    2|\n",
      "| N36472|    4|\n",
      "| N38451|    4|\n",
      "| N73283|    4|\n",
      "| N513UA|    2|\n",
      "| N954WN|    5|\n",
      "| N388DA|    3|\n",
      "| N567AA|    1|\n",
      "| N516UA|    2|\n",
      "| N927DN|    1|\n",
      "| N8322X|    1|\n",
      "| N466SW|    1|\n",
      "|  N6700|    1|\n",
      "| N607AS|   45|\n",
      "| N622SW|    4|\n",
      "| N584AS|   31|\n",
      "| N914WN|    4|\n",
      "| N654AW|    2|\n",
      "| N336NW|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   SEA| 160.4361496051259|\n",
      "|   PDX|137.11543248288737|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy(\"tailnum\")\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show()\n",
    "\n",
    "# Group by origin\n",
    "by_origin = flights.groupBy(\"origin\")\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|month|dest|      avg(dep_delay)|\n",
      "+-----+----+--------------------+\n",
      "|   11| TUS| -2.3333333333333335|\n",
      "|   11| ANC|   7.529411764705882|\n",
      "|    1| BUR|               -1.45|\n",
      "|    1| PDX| -5.6923076923076925|\n",
      "|    6| SBA|                -2.5|\n",
      "|    5| LAX|-0.15789473684210525|\n",
      "|   10| DTW|                 2.6|\n",
      "|    6| SIT|                -1.0|\n",
      "|   10| DFW|  18.176470588235293|\n",
      "|    3| FAI|                -2.2|\n",
      "|   10| SEA|                -0.8|\n",
      "|    2| TUS| -0.6666666666666666|\n",
      "|   12| OGG|  25.181818181818183|\n",
      "|    9| DFW|   4.066666666666666|\n",
      "|    5| EWR|               14.25|\n",
      "|    3| RDM|                -6.2|\n",
      "|    8| DCA|                 2.6|\n",
      "|    7| ATL|   4.675675675675675|\n",
      "|    4| JFK| 0.07142857142857142|\n",
      "|   10| SNA| -1.1333333333333333|\n",
      "+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|   11| TUS|    3.0550504633038935|\n",
      "|   11| ANC|    18.604716401245316|\n",
      "|    1| BUR|     15.22627576540667|\n",
      "|    1| PDX|     5.677214918493858|\n",
      "|    6| SBA|     2.380476142847617|\n",
      "|    5| LAX|     13.36268698685904|\n",
      "|   10| DTW|     5.639148871948674|\n",
      "|    6| SIT|                   NaN|\n",
      "|   10| DFW|     45.53019017606675|\n",
      "|    3| FAI|    3.1144823004794873|\n",
      "|   10| SEA|     18.70523227029577|\n",
      "|    2| TUS|    14.468356276140469|\n",
      "|   12| OGG|     82.64480404939947|\n",
      "|    9| DFW|    21.728629347782924|\n",
      "|    5| EWR|     42.41595968929191|\n",
      "|    3| RDM|      2.16794833886788|\n",
      "|    8| DCA|     9.946523680831074|\n",
      "|    7| ATL|    22.767001039582183|\n",
      "|    4| JFK|     8.156774303176903|\n",
      "|   10| SNA|    13.726234873756304|\n",
      "+-----+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg(\"dep_delay\").show()\n",
    "\n",
    "# Standard deviation\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|faa|                name|             lat|              lon| alt| tz|dst|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
      "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
      "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
      "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
      "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
      "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
      "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
      "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
      "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
      "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
      "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
      "+---+--------------------+----------------+-----------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------+-----------+---+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|              name|      lat|        lon|alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------+-----------+---+---+---+\n",
      "| LAX|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA|     132|     954|   6|    58|  Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA|     360|    2677|  10|    40|     Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA|     111|     679|  14|    43|San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
    "\n",
    "# Examine the data again\n",
    "print(flights_with_airports.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dest: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      " |-- alt: string (nullable = true)\n",
      " |-- tz: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_with_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|                name|      lat|        lon| alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+\n",
      "| LAX|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA|     132|     954|   6|    58|    Los Angeles Intl|33.942536|-118.408075| 126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA|     360|    2677|  10|    40|       Honolulu Intl|21.318681|-157.922428|  13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA|     111|     679|  14|    43|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|\n",
      "| SJC|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX|      83|     569|  17|     5|Norman Y Mineta S...|  37.3626|-121.929022|  62| -8|  A|\n",
      "| BUR|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA|     127|     937|   7|    54|            Bob Hope|34.200667|-118.358667| 778| -8|  A|\n",
      "| DEN|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX|     121|     991|  10|    37|         Denver Intl|39.861656|-104.673178|5431| -7|  A|\n",
      "| OAK|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX|      90|     543|   8|    47|Metropolitan Oakl...|37.721278|-122.220722|   9| -8|  A|\n",
      "| SFO|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA|      98|     679|  16|    55|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|\n",
      "| SAN|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA|     135|    1050|  12|    36|      San Diego Intl|32.733556|-117.189667|  17| -8|  A|\n",
      "| ORD|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA|     198|    1721|  18|    12|  Chicago Ohare Intl|41.978603| -87.904842| 668| -6|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_with_airports.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('196', '242'), ('186', '302'), ('22', '377'), ('244', '51'), ('166', '346')]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename=\"/Users/vkocaman/Python_Projects/Leiden/Hadoop/ml-100k/u.data\"\n",
    "\n",
    "movie_rdd=sc.parallelize([ (x[0],x[1]) for x in csv.reader(open(filename,'r'),delimiter='\\t')])\n",
    "\n",
    "movie_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss.read.csv(\"/Users/vkocaman/Python_Projects/Leiden/Hadoop/ml-100k/data.csv\", inferSchema=True, header=None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(737, '405'),\n",
       " (685, '655'),\n",
       " (636, '13'),\n",
       " (540, '450'),\n",
       " (518, '276'),\n",
       " (493, '416'),\n",
       " (490, '537'),\n",
       " (484, '303'),\n",
       " (480, '234'),\n",
       " (448, '393')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count example\n",
    "\n",
    "counts_rdd = movie_rdd.map(lambda word: (word[0], 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# print the word frequencies in descending order\n",
    "\n",
    "counts_rdd.map(lambda x: (x[1], x[0])) \\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .collect()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[a[0] for a in [ (x[0],x[1]) for x in csv.reader(open(filename,'r'),delimiter='\\t')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max(Counter(m).values())\n",
    "\n",
    "Counter(m)[\"405\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with machine learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"example_text.txt\")\n",
    "wordSeqs = text_file.map(lambda s: [w.lower() for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "becomes: 0.9162028431892395\n",
      "fish: 0.9139411449432373\n",
      "enough: 0.9098803997039795\n",
      "years.: 0.8977555632591248\n",
      "wit,: 0.8932639956474304\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "w2v = Word2Vec()\n",
    "model = w2v.fit(wordSeqs)\n",
    "\n",
    "# find synonyms for a given word\n",
    "synonyms = model.findSynonyms('money', 5)\n",
    "\n",
    "for word, distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/radanalyticsio/workshop-notebook/blob/master/pyspark.ipynb\n",
    "\n",
    "# https://github.com/radanalyticsio/workshop-notebook/blob/master/ml-basics.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARK ML PART IS ABBREVIATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional resources\n",
    "\n",
    "https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64\n",
    "\n",
    "https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a\n",
    "\n",
    "https://medium.com/@mrpowers/manually-creating-spark-dataframes-b14dae906393\n",
    "\n",
    "https://www.youtube.com/watch?v=QaoJNXW6SQo\n",
    "(Spark Tutorial For Beginners | Big Data Spark Tutorial | Apache Spark Tutorial | Simplilearn)\n",
    "\n",
    "Querying large dataset with PySpark SQL from S3 on Local Jupyter Notebook\n",
    "https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://blog.usejournal.com/spark-study-notes-core-concepts-visualized-5256c44e4090\n",
    "\n",
    "https://data-flair.training/blogs/spark-tutorial/\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "https://nbviewer.jupyter.org/github/mepa/sads-pyspark/blob/master/2017-09-14-PySpark-Workshop.slides.html\n",
    "\n",
    "https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473\n",
    "\n",
    "Spark Web UI\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-webui.html\n",
    "\n",
    "And some more detail about UI >> \n",
    "https://blog.insightdatascience.com/using-jupyter-on-apache-spark-step-by-step-with-a-terabyte-of-reddit-data-ef4d6c13959a\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "https://towardsdatascience.com/how-does-apache-spark-run-on-a-cluster-974ec2731f20\n",
    "\n",
    "https://blog.usejournal.com/spark-study-notes-core-concepts-visualized-5256c44e4090\n",
    "\n",
    "https://stackoverflow.com/questions/32356143/what-does-setmaster-local-mean-in-spark\n",
    "\n",
    "https://techvidvan.com/tutorials/spark-modes-of-deployment/\n",
    "\n",
    "https://medium.com/@mrpowers/working-with-spark-arraytype-and-maptype-columns-4d85f3c8b2b3\n",
    "\n",
    "Spark Streaming\n",
    "\n",
    "Test with netcat local data server (https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "https://engineering.billymob.com/introducing-spark-streaming-c1b8be36c775\n",
    "\n",
    "https://engineering.billymob.com/apache-spark-streaming-kafka-0-10-1f3c29a694cb\n",
    "\n",
    "https://engineering.billymob.com/feature-integrating-kafka-with-spark-streaming-47763f6bcf58\n",
    "\n",
    "https://medium.com/@kass09/spark-streaming-kafka-in-python-a-test-on-local-machine-edd47814746\n",
    "\n",
    "*https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/  (two diff version of application-- windowed vs batch)\n",
    "\n",
    "(Getting+Started+with+Spark+Streaming+with+Python+and+Kafka.ipynb)\n",
    "\n",
    "Part-2 : (write filtered tweets to another kafka topic)\n",
    "https://www.rittmanmead.com/blog/2017/01/data-processing-and-enrichment-in-spark-streaming-with-python-and-kafka/\n",
    "\n",
    "https://medium.com/@mukeshkumar_46704/getting-streaming-data-from-kafka-with-spark-streaming-using-python-9cd0922fa904\n",
    "\n",
    "Full code\n",
    "https://gist.github.com/rmoff/fb033086b285655ffe7f9ff0582dedbf\n",
    "\n",
    "http://tlfvincent.github.io/2016/09/25/kafka-spark-pipeline-part-1/\n",
    "\n",
    "https://www.supergloo.com/fieldnotes/spark-streaming-kafka-example/\n",
    "\n",
    "https://www.opcito.com/blogs/building-a-real-time-data-pipeline-using-spark-streaming-and-kafka/\n",
    "\n",
    "https://www.opcito.com/blogs/data-ingestion-with-hadoop-yarn-spark-and-kafka/\n",
    "\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at your local machine \n",
    "\n",
    "https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617\n",
    "\n",
    "https://medium.freecodecamp.org/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389\n",
    "\n",
    "https://github.com/tirthajyoti/Spark-with-Python\n",
    "\n",
    "Run PySpark with Docker at your local machine\n",
    "\n",
    "https://levelup.gitconnected.com/using-docker-and-pyspark-134cd4cab867\n",
    "\n",
    "https://medium.com/@suci/running-pyspark-on-jupyter-notebook-with-docker-602b18ac4494\n",
    "\n",
    "https://medium.com/@GaryStafford/getting-started-with-pyspark-for-big-data-analytics-using-jupyter-notebooks-and-docker-ba39d2e3d6c7 (including Postgres db)\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at AWS EC2 \n",
    "\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "\n",
    "\n",
    "Install and run PySpark on AWS EMR (with Hadoop and Spark pre-installed)\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-distributed-ml-using-aws-emr-apache-spark-pyspark-and-mongodb-tutorial-with-4d1077f68381 \n",
    "\n",
    "https://medium.com/@datitran/quickstart-pyspark-with-anaconda-on-aws-660252b88c9a\n",
    "\n",
    "**https://medium.com/idealo-tech-blog/using-terraform-to-quick-start-pyspark-on-aws-2bc8ce9dcac\n",
    "\n",
    "Install and run PySpark on Jupyter Notebook at GCP DataProc\n",
    "\thttps://towardsdatascience.com/data-science-for-startups-pyspark-1acf51e9d6ba\n",
    "\n",
    "https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud\n",
    "\n",
    "Submit PySpark jobs on GCP DataProc (with Hadoop and Spark pre-installed)\n",
    "\n",
    "https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468\n",
    "\n",
    "Run PySpark on DSLab Machines\n",
    "\n",
    "Running Spark clusters on Databricks Community Edition (just let the students know that this is another option.. no need to delve into)\n",
    "\n",
    "Spark MLlib (ML with PySpark)\n",
    "\n",
    "https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "\n",
    "Data cleaning >> https://github.com/radanalyticsio/workshop-notebook/blob/master/workshop.ipynb\n",
    "\n",
    "https://github.com/radanalyticsio/workshop-notebook/blob/master/ml-basics.ipynb\n",
    "\n",
    "Deploying PySpark ML Model on Google Compute Engine as a REST API\n",
    "\n",
    "https://towardsdatascience.com/deploying-pyspark-ml-model-on-google-compute-engine-as-a-rest-api-d69e126b30b1\n",
    "\n",
    "\n",
    "https://www.linkedin.com/pulse/insider-spark-adventure-bar%C4%B1%C5%9F-can-tayiz/\n",
    "    \n",
    "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa?gi=165a78ac88a\n",
    "\n",
    "### technicalities of Spark\n",
    "\n",
    "how-does-apache-spark-run-on-a-cluster\n",
    "\n",
    "https://towardsdatascience.com/how-does-apache-spark-run-on-a-cluster-974ec2731f20\n",
    "\n",
    "https://spark.apache.org/docs/latest/spark-standalone.html\n",
    "    \n",
    "http://devopspy.com/python/apache-spark-pyspark-centos-rhel/\n",
    "    \n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-webui.html\n",
    "    \n",
    "https://medium.com/ymedialabs-innovation/apache-spark-on-a-multi-node-cluster-b75967c8cb2b\n",
    "    \n",
    "https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-standalone-example-2-workers-on-1-node-cluster.adoc\n",
    "    \n",
    "http://spark.apache.org/docs/latest/submitting-applications.html\n",
    "        \n",
    "https://docs.anaconda.com/anaconda-scale/howto/spark-basic/\n",
    "    \n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "    \n",
    "https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/\n",
    "    \n",
    "https://www.programcreek.com/2018/11/install-spark-on-ubuntu-standalone-mode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
